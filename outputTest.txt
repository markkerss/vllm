INFO 07-23 06:18:49 [__init__.py:239] Automatically detected platform cuda.
INFO 07-23 06:18:58 [config.py:604] This model supports multiple tasks: {'classify', 'reward', 'generate', 'embed', 'score'}. Defaulting to 'generate'.
WARNING 07-23 06:18:58 [arg_utils.py:1607] Chunked prefill is enabled by default for models with max_model_len > 32K. Chunked prefill might not work with some features or models. If you encounter any issues, please disable by launching with --enable-chunked-prefill=False.
INFO 07-23 06:18:58 [config.py:1797] Chunked prefill is enabled with max_num_batched_tokens=2048.
INFO 07-23 06:18:58 [llm_engine.py:249] Initializing a V0 LLM engine (v0.1.dev5719+g2fe1487) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 07-23 06:19:01 [cuda.py:292] Using Flash Attention backend.
INFO 07-23 06:19:01 [parallel_state.py:957] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 07-23 06:19:01 [model_runner.py:1171] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 07-23 06:19:01 [weight_utils.py:265] Using model weights format ['*.safetensors']
INFO 07-23 06:19:05 [loader.py:458] Loading weights took 3.14 seconds
INFO 07-23 06:19:05 [model_runner.py:1207] Model loading took 14.9889 GiB and 3.696235 seconds
INFO 07-23 06:19:05 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='0', is_prompt=True, seq_data={0: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='1', is_prompt=True, seq_data={1: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='2', is_prompt=True, seq_data={2: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='3', is_prompt=True, seq_data={3: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='4', is_prompt=True, seq_data={4: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='5', is_prompt=True, seq_data={5: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='6', is_prompt=True, seq_data={6: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='7', is_prompt=True, seq_data={7: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='8', is_prompt=True, seq_data={8: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='9', is_prompt=True, seq_data={9: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='10', is_prompt=True, seq_data={10: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='11', is_prompt=True, seq_data={11: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='12', is_prompt=True, seq_data={12: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='13', is_prompt=True, seq_data={13: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='14', is_prompt=True, seq_data={14: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='15', is_prompt=True, seq_data={15: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='16', is_prompt=True, seq_data={16: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='17', is_prompt=True, seq_data={17: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='18', is_prompt=True, seq_data={18: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='19', is_prompt=True, seq_data={19: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='20', is_prompt=True, seq_data={20: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='21', is_prompt=True, seq_data={21: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='22', is_prompt=True, seq_data={22: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='23', is_prompt=True, seq_data={23: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='24', is_prompt=True, seq_data={24: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='25', is_prompt=True, seq_data={25: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='26', is_prompt=True, seq_data={26: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='27', is_prompt=True, seq_data={27: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='28', is_prompt=True, seq_data={28: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='29', is_prompt=True, seq_data={29: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='30', is_prompt=True, seq_data={30: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='31', is_prompt=True, seq_data={31: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='32', is_prompt=True, seq_data={32: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='33', is_prompt=True, seq_data={33: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='34', is_prompt=True, seq_data={34: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='35', is_prompt=True, seq_data={35: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='36', is_prompt=True, seq_data={36: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='37', is_prompt=True, seq_data={37: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='38', is_prompt=True, seq_data={38: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='39', is_prompt=True, seq_data={39: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='40', is_prompt=True, seq_data={40: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='41', is_prompt=True, seq_data={41: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='42', is_prompt=True, seq_data={42: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='43', is_prompt=True, seq_data={43: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='44', is_prompt=True, seq_data={44: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='45', is_prompt=True, seq_data={45: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='46', is_prompt=True, seq_data={46: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='47', is_prompt=True, seq_data={47: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='48', is_prompt=True, seq_data={48: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='49', is_prompt=True, seq_data={49: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='50', is_prompt=True, seq_data={50: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='51', is_prompt=True, seq_data={51: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='52', is_prompt=True, seq_data={52: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='53', is_prompt=True, seq_data={53: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='54', is_prompt=True, seq_data={54: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='55', is_prompt=True, seq_data={55: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='56', is_prompt=True, seq_data={56: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='57', is_prompt=True, seq_data={57: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='58', is_prompt=True, seq_data={58: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='59', is_prompt=True, seq_data={59: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='60', is_prompt=True, seq_data={60: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='61', is_prompt=True, seq_data={61: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='62', is_prompt=True, seq_data={62: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='63', is_prompt=True, seq_data={63: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='64', is_prompt=True, seq_data={64: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='65', is_prompt=True, seq_data={65: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='66', is_prompt=True, seq_data={66: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='67', is_prompt=True, seq_data={67: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='68', is_prompt=True, seq_data={68: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='69', is_prompt=True, seq_data={69: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='70', is_prompt=True, seq_data={70: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='71', is_prompt=True, seq_data={71: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='72', is_prompt=True, seq_data={72: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='73', is_prompt=True, seq_data={73: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='74', is_prompt=True, seq_data={74: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='75', is_prompt=True, seq_data={75: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='76', is_prompt=True, seq_data={76: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='77', is_prompt=True, seq_data={77: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='78', is_prompt=True, seq_data={78: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='79', is_prompt=True, seq_data={79: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='80', is_prompt=True, seq_data={80: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='81', is_prompt=True, seq_data={81: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='82', is_prompt=True, seq_data={82: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='83', is_prompt=True, seq_data={83: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='84', is_prompt=True, seq_data={84: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='85', is_prompt=True, seq_data={85: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='86', is_prompt=True, seq_data={86: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='87', is_prompt=True, seq_data={87: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='88', is_prompt=True, seq_data={88: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='89', is_prompt=True, seq_data={89: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='90', is_prompt=True, seq_data={90: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='91', is_prompt=True, seq_data={91: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='92', is_prompt=True, seq_data={92: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='93', is_prompt=True, seq_data={93: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='94', is_prompt=True, seq_data={94: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='95', is_prompt=True, seq_data={95: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='96', is_prompt=True, seq_data={96: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='97', is_prompt=True, seq_data={97: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='98', is_prompt=True, seq_data={98: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='99', is_prompt=True, seq_data={99: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='100', is_prompt=True, seq_data={100: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='101', is_prompt=True, seq_data={101: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='102', is_prompt=True, seq_data={102: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='103', is_prompt=True, seq_data={103: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='104', is_prompt=True, seq_data={104: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='105', is_prompt=True, seq_data={105: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='106', is_prompt=True, seq_data={106: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='107', is_prompt=True, seq_data={107: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='108', is_prompt=True, seq_data={108: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='109', is_prompt=True, seq_data={109: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='110', is_prompt=True, seq_data={110: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='111', is_prompt=True, seq_data={111: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='112', is_prompt=True, seq_data={112: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='113', is_prompt=True, seq_data={113: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='114', is_prompt=True, seq_data={114: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='115', is_prompt=True, seq_data={115: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='116', is_prompt=True, seq_data={116: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='117', is_prompt=True, seq_data={117: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='118', is_prompt=True, seq_data={118: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='119', is_prompt=True, seq_data={119: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='120', is_prompt=True, seq_data={120: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='121', is_prompt=True, seq_data={121: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='122', is_prompt=True, seq_data={122: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='123', is_prompt=True, seq_data={123: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='124', is_prompt=True, seq_data={124: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='125', is_prompt=True, seq_data={125: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='126', is_prompt=True, seq_data={126: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='127', is_prompt=True, seq_data={127: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='128', is_prompt=True, seq_data={128: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='129', is_prompt=True, seq_data={129: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='130', is_prompt=True, seq_data={130: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='131', is_prompt=True, seq_data={131: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='132', is_prompt=True, seq_data={132: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='133', is_prompt=True, seq_data={133: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='134', is_prompt=True, seq_data={134: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='135', is_prompt=True, seq_data={135: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='136', is_prompt=True, seq_data={136: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='137', is_prompt=True, seq_data={137: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='138', is_prompt=True, seq_data={138: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='139', is_prompt=True, seq_data={139: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='140', is_prompt=True, seq_data={140: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='141', is_prompt=True, seq_data={141: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='142', is_prompt=True, seq_data={142: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='143', is_prompt=True, seq_data={143: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='144', is_prompt=True, seq_data={144: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='145', is_prompt=True, seq_data={145: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='146', is_prompt=True, seq_data={146: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='147', is_prompt=True, seq_data={147: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='148', is_prompt=True, seq_data={148: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='149', is_prompt=True, seq_data={149: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='150', is_prompt=True, seq_data={150: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='151', is_prompt=True, seq_data={151: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='152', is_prompt=True, seq_data={152: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='153', is_prompt=True, seq_data={153: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='154', is_prompt=True, seq_data={154: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='155', is_prompt=True, seq_data={155: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='156', is_prompt=True, seq_data={156: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='157', is_prompt=True, seq_data={157: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='158', is_prompt=True, seq_data={158: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='159', is_prompt=True, seq_data={159: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='160', is_prompt=True, seq_data={160: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='161', is_prompt=True, seq_data={161: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='162', is_prompt=True, seq_data={162: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='163', is_prompt=True, seq_data={163: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='164', is_prompt=True, seq_data={164: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='165', is_prompt=True, seq_data={165: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='166', is_prompt=True, seq_data={166: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='167', is_prompt=True, seq_data={167: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='168', is_prompt=True, seq_data={168: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='169', is_prompt=True, seq_data={169: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='170', is_prompt=True, seq_data={170: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='171', is_prompt=True, seq_data={171: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='172', is_prompt=True, seq_data={172: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='173', is_prompt=True, seq_data={173: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='174', is_prompt=True, seq_data={174: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='175', is_prompt=True, seq_data={175: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='176', is_prompt=True, seq_data={176: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='177', is_prompt=True, seq_data={177: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='178', is_prompt=True, seq_data={178: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='179', is_prompt=True, seq_data={179: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='180', is_prompt=True, seq_data={180: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='181', is_prompt=True, seq_data={181: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='182', is_prompt=True, seq_data={182: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='183', is_prompt=True, seq_data={183: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='184', is_prompt=True, seq_data={184: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='185', is_prompt=True, seq_data={185: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='186', is_prompt=True, seq_data={186: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='187', is_prompt=True, seq_data={187: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='188', is_prompt=True, seq_data={188: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='189', is_prompt=True, seq_data={189: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='190', is_prompt=True, seq_data={190: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='191', is_prompt=True, seq_data={191: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='192', is_prompt=True, seq_data={192: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='193', is_prompt=True, seq_data={193: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='194', is_prompt=True, seq_data={194: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='195', is_prompt=True, seq_data={195: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='196', is_prompt=True, seq_data={196: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='197', is_prompt=True, seq_data={197: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='198', is_prompt=True, seq_data={198: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='199', is_prompt=True, seq_data={199: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='200', is_prompt=True, seq_data={200: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='201', is_prompt=True, seq_data={201: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='202', is_prompt=True, seq_data={202: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='203', is_prompt=True, seq_data={203: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='204', is_prompt=True, seq_data={204: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='205', is_prompt=True, seq_data={205: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='206', is_prompt=True, seq_data={206: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='207', is_prompt=True, seq_data={207: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='208', is_prompt=True, seq_data={208: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='209', is_prompt=True, seq_data={209: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='210', is_prompt=True, seq_data={210: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='211', is_prompt=True, seq_data={211: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='212', is_prompt=True, seq_data={212: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='213', is_prompt=True, seq_data={213: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='214', is_prompt=True, seq_data={214: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='215', is_prompt=True, seq_data={215: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='216', is_prompt=True, seq_data={216: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='217', is_prompt=True, seq_data={217: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='218', is_prompt=True, seq_data={218: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='219', is_prompt=True, seq_data={219: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='220', is_prompt=True, seq_data={220: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='221', is_prompt=True, seq_data={221: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='222', is_prompt=True, seq_data={222: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='223', is_prompt=True, seq_data={223: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='224', is_prompt=True, seq_data={224: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='225', is_prompt=True, seq_data={225: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='226', is_prompt=True, seq_data={226: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='227', is_prompt=True, seq_data={227: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='228', is_prompt=True, seq_data={228: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='229', is_prompt=True, seq_data={229: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='230', is_prompt=True, seq_data={230: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='231', is_prompt=True, seq_data={231: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='232', is_prompt=True, seq_data={232: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='233', is_prompt=True, seq_data={233: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='234', is_prompt=True, seq_data={234: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='235', is_prompt=True, seq_data={235: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='236', is_prompt=True, seq_data={236: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='237', is_prompt=True, seq_data={237: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='238', is_prompt=True, seq_data={238: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='239', is_prompt=True, seq_data={239: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='240', is_prompt=True, seq_data={240: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='241', is_prompt=True, seq_data={241: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='242', is_prompt=True, seq_data={242: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='243', is_prompt=True, seq_data={243: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='244', is_prompt=True, seq_data={244: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='245', is_prompt=True, seq_data={245: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='246', is_prompt=True, seq_data={246: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='247', is_prompt=True, seq_data={247: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='248', is_prompt=True, seq_data={248: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='249', is_prompt=True, seq_data={249: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='250', is_prompt=True, seq_data={250: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='251', is_prompt=True, seq_data={251: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='252', is_prompt=True, seq_data={252: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='253', is_prompt=True, seq_data={253: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='254', is_prompt=True, seq_data={254: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='255', is_prompt=True, seq_data={255: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None)]
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 0, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 0 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 0
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 1, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 1 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
DEBUG: _compute_lens for req_id 1, seq_id 1
  context_len: 0, seq_len (for step): 8, token_chunk_size: 8
  Calculated tokens for this step: [0, 0, 0, 0, 0, 0, 0, 0] (len: 8)
  Calculated positions for this step: list(range(0, 8)) (len: 8)
  Positions: []
added seq group 1
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 2, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 2 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 2
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 3, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 3 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 3
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 4, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 4 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 4
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 5, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 5 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 5
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 6, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 6 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 6
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 7, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 7 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 7
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 8, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 8 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 8
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 9, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 9 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 9
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 10, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 10 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 10
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 11, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 11 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 11
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 12, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 12 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 12
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 13, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 13 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 13
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 14, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 14 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 14
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 15, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 15 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 15
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 16, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 16 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 16
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 17, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 17 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 17
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 18, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 18 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 18
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 19, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 19 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 19
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 20, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 20 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 20
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 21, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 21 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 21
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 22, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 22 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 22
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 23, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 23 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 23
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 24, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 24 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 24
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 25, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 25 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 25
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 26, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 26 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 26
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 27, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 27 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 27
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 28, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 28 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 28
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 29, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 29 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 29
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 30, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 30 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 30
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 31, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 31 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 31
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 32, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 32 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 32
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 33, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 33 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 33
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 34, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 34 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 34
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 35, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 35 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 35
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 36, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 36 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 36
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 37, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 37 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 37
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 38, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 38 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 38
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 39, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 39 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 39
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 40, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 40 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 40
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 41, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 41 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 41
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 42, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 42 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 42
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 43, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 43 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 43
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 44, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 44 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 44
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 45, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 45 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 45
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 46, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 46 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 46
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 47, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 47 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 47
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 48, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 48 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 48
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 49, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 49 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 49
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 50, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 50 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 50
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 51, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 51 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 51
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 52, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 52 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 52
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 53, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 53 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 53
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 54, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 54 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 54
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 55, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 55 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 55
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 56, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 56 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 56
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 57, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 57 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 57
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 58, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 58 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 58
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 59, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 59 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 59
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 60, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 60 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 60
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 61, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 61 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 61
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 62, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 62 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 62
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 63, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 63 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 63
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 64, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 64 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 64
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 65, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 65 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 65
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 66, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 66 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 66
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 67, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 67 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 67
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 68, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 68 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 68
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 69, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 69 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 69
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 70, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 70 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 70
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 71, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 71 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 71
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 72, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 72 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 72
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 73, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 73 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 73
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 74, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 74 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 74
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 75, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 75 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 75
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 76, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 76 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 76
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 77, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 77 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 77
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 78, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 78 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 78
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 79, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 79 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 79
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 80, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 80 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 80
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 81, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 81 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 81
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 82, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 82 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 82
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 83, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 83 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 83
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 84, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 84 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 84
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 85, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 85 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 85
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 86, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 86 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 86
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 87, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 87 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 87
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 88, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 88 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 88
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 89, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 89 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 89
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 90, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 90 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 90
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 91, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 91 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 91
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 92, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 92 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 92
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 93, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 93 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 93
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 94, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 94 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 94
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 95, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 95 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 95
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 96, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 96 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 96
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 97, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 97 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 97
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 98, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 98 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 98
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 99, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 99 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 99
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 100, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 100 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 100
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 101, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 101 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 101
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 102, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 102 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 102
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 103, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 103 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 103
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 104, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 104 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 104
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 105, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 105 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 105
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 106, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 106 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 106
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 107, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 107 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 107
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 108, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 108 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 108
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 109, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 109 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 109
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 110, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 110 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 110
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 111, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 111 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 111
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 112, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 112 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 112
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 113, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 113 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 113
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 114, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 114 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 114
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 115, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 115 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 115
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 116, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 116 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 116
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 117, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 117 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 117
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 118, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 118 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 118
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 119, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 119 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 119
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 120, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 120 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 120
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 121, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 121 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 121
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 122, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 122 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 122
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 123, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 123 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 123
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 124, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 124 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 124
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 125, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 125 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 125
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 126, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 126 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 126
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 127, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 127 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 127
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 128, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 128 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 128
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 129, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 129 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 129
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 130, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 130 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 130
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 131, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 131 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 131
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 132, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 132 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 132
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 133, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 133 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 133
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 134, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 134 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 134
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 135, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 135 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 135
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 136, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 136 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 136
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 137, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 137 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 137
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 138, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 138 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 138
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 139, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 139 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 139
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 140, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 140 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 140
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 141, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 141 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 141
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 142, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 142 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 142
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 143, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 143 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 143
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 144, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 144 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 144
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 145, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 145 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 145
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 146, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 146 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 146
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 147, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 147 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 147
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 148, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 148 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 148
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 149, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 149 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 149
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 150, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 150 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 150
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 151, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 151 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 151
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 152, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 152 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 152
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 153, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 153 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 153
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 154, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 154 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 154
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 155, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 155 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 155
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 156, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 156 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 156
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 157, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 157 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 157
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 158, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 158 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 158
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 159, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 159 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 159
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 160, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 160 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 160
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 161, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 161 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 161
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 162, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 162 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 162
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 163, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 163 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 163
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 164, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 164 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 164
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 165, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 165 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 165
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 166, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 166 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 166
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 167, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 167 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 167
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 168, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 168 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 168
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 169, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 169 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 169
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 170, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 170 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 170
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 171, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 171 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 171
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 172, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 172 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 172
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 173, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 173 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 173
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 174, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 174 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 174
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 175, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 175 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 175
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 176, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 176 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 176
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 177, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 177 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 177
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 178, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 178 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 178
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 179, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 179 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 179
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 180, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 180 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 180
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 181, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 181 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 181
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 182, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 182 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 182
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 183, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 183 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 183
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 184, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 184 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 184
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 185, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 185 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 185
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 186, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 186 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 186
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 187, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 187 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 187
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 188, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 188 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 188
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 189, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 189 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 189
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 190, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 190 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 190
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 191, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 191 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 191
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 192, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 192 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 192
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 193, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 193 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 193
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 194, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 194 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 194
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 195, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 195 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 195
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 196, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 196 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 196
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 197, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 197 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 197
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 198, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 198 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 198
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 199, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 199 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 199
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 200, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 200 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 200
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 201, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 201 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 201
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 202, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 202 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 202
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 203, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 203 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 203
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 204, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 204 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 204
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 205, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 205 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 205
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 206, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 206 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 206
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 207, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 207 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 207
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 208, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 208 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 208
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 209, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 209 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 209
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 210, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 210 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 210
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 211, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 211 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 211
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 212, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 212 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 212
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 213, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 213 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 213
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 214, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 214 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 214
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 215, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 215 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 215
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 216, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 216 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 216
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 217, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 217 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 217
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 218, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 218 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 218
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 219, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 219 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 219
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 220, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 220 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 220
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 221, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 221 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 221
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 222, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 222 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 222
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 223, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 223 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 223
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 224, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 224 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 224
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 225, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 225 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 225
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 226, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 226 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 226
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 227, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 227 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 227
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 228, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 228 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 228
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 229, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 229 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 229
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 230, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 230 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 230
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 231, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 231 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 231
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 232, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 232 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 232
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 233, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 233 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 233
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 234, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 234 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 234
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 235, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 235 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 235
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 236, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 236 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 236
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 237, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 237 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 237
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 238, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 238 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 238
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 239, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 239 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 239
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 240, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 240 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 240
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 241, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 241 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 241
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 242, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 242 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 242
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 243, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 243 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 243
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 244, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 244 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 244
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 245, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 245 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 245
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 246, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 246 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 246
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 247, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 247 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 247
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 248, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 248 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 248
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 249, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 249 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 249
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 250, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 250 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 250
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 251, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 251 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 251
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 252, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 252 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 252
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 253, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 253 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 253
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 254, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 254 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 254
INFO 07-23 06:19:05 [model_runner.py:1284] Processing seq group 255, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 255 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[0, 0, 0, 0, 0, 0, 0, 0], positions=[0, 1, 2, 3, 4, 5, 6, 7]
added seq group 255
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]... (len: 2048)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [0, 1, 2, 3, 4, 5, 6, 7, 0, 1]... (len: 2048)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 1):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [0, 0, 0, 0, 0, 0, 0, 0] (len: 8)
      Seq 0: input_positions in list: [0, 1, 2, 3, 4, 5, 6, 7] (len: 8)
      Seq 0: Expected segment in FLAT input_tokens: offset 8, len 8
      Seq 0: Expected segment in FLAT input_positions: offset 8, len 8
  Total flattened input_tokens (pre-padding): [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] (len: 2048)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([2048])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([2048])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
INFO 07-23 06:19:06 [worker.py:269] Memory profiling takes 0.88 seconds
INFO 07-23 06:19:06 [worker.py:269] the current vLLM instance can use total_gpu_memory (79.15GiB) x gpu_memory_utilization (0.90) = 71.24GiB
INFO 07-23 06:19:06 [worker.py:269] model weights take 14.99GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 1.19GiB; the rest of the memory reserved for KV Cache is 54.97GiB.
INFO 07-23 06:19:06 [executor_base.py:112] # cuda blocks: 28143, # CPU blocks: 2048
INFO 07-23 06:19:06 [executor_base.py:117] Maximum concurrency for 131072 tokens per request: 3.44x
INFO 07-23 06:19:08 [model_runner.py:1528] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 07-23 06:19:21 [model_runner.py:1670] Graph capturing finished in 13 secs, took 0.88 GiB
INFO 07-23 06:19:21 [llm_engine.py:455] init engine (profile, create kv cache, warmup model) took 15.82 seconds
run_first_add_chunk
SCHEDULING NOT IMPORTED
New tokens to schedule 9
ALLOCATED AND SET RUNNING
SCEDULING THE GROUP
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220)
INFO 07-23 06:19:21 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=True, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=[], multi_modal_data={}, multi_modal_placeholders={}, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=9, num_speculative_tokens=None)]
INFO 07-23 06:19:21 [model_runner.py:1284] Processing seq group 1, is_prompt=True, multi_modal_data=True
MAX TOKENS 1
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220)
TOKEN CHUNK SIZE 9
SEQ_DATA INSIDE WORKER 0 9 [128000, 1271, 387, 477, 539, 311, 387, 11, 220]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[128000, 1271, 387, 477, 539, 311, 387, 11, 220], positions=[0, 1, 2, 3, 4, 5, 6, 7, 8]
DEBUG: _compute_lens for req_id 1, seq_id 0
  context_len: 0, seq_len (for step): 9, token_chunk_size: 9
  Calculated tokens for this step: [128000, 1271, 387, 477, 539, 311, 387, 11, 220] (len: 9)
  Calculated positions for this step: list(range(0, 9)) (len: 9)
  Positions: []
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [128000, 1271, 387, 477, 539, 311, 387, 11, 220]... (len: 9)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [0, 1, 2, 3, 4, 5, 6, 7, 8]... (len: 9)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [128000, 1271, 387, 477, 539, 311, 387, 11, 220] (len: 9)
      Seq 0: input_positions in list: [0, 1, 2, 3, 4, 5, 6, 7, 8] (len: 9)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 9
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 9
  Total flattened input_tokens (pre-padding): [128000, 1271, 387, 477, 539, 311, 387, 11, 220] (len: 9)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([9])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([9])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 0
CALLING RPC
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220)
INFO 07-23 06:19:22 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220]), output_token_ids=(17,), cumulative_logprob=inf, get_num_computed_tokens=9)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:19:22 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 1
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 9 10 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 17]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[17], positions=[9]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [17]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [9]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [17] (len: 1)
      Seq 0: input_positions in list: [9] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [17] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 1
INFO 07-23 06:19:26 [metrics.py:489] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 0.2 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
run_add_chunk
Request id 1
Suspended requests {'1': SequenceGroup(request_id=1, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), num_seqs=1)}
WE ARE ADDING [128000, 9210, 374, 279, 220] TO SEQ ID 0
COMPUTED TOKENS: 8 
OUTPUT LEN: 0 
LEN: 14 
PROMPT LEN: 14
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
New tokens to schedule 6
SEQUENCE IS PREFILL True
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
CHECKING SCHEDULED SEQ GROUP CONTENTS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SCHEDULED SEQ GROUP CONTENTS (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SCHEDULING PREFILL
RUNNING GROUP PREFILLS BEFORE RETURN (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
RUNNING GROUP PREFILLS (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:19:26 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=True, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(), cumulative_logprob=inf, get_num_computed_tokens=8)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.PREFILL: 1>, token_type_ids=[], multi_modal_data={}, multi_modal_placeholders={}, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=6, num_speculative_tokens=None)]
INFO 07-23 06:19:26 [model_runner.py:1284] Processing seq group 1, is_prompt=True, multi_modal_data=True
MAX TOKENS 1
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 6
SEQ_DATA INSIDE WORKER 8 14 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[220, 128000, 9210, 374, 279, 220], positions=[8, 9, 10, 11, 12, 13]
DEBUG: _compute_lens for req_id 1, seq_id 0
  context_len: 8, seq_len (for step): 14, token_chunk_size: 6
  Calculated tokens for this step: [220, 128000, 9210, 374, 279, 220] (len: 6)
  Calculated positions for this step: list(range(8, 14)) (len: 6)
  Positions: []
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [220, 128000, 9210, 374, 279, 220]... (len: 6)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [8, 9, 10, 11, 12, 13]... (len: 6)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [220, 128000, 9210, 374, 279, 220] (len: 6)
      Seq 0: input_positions in list: [8, 9, 10, 11, 12, 13] (len: 6)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 6
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 6
  Total flattened input_tokens (pre-padding): [220, 128000, 9210, 374, 279, 220] (len: 6)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([6])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([6])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 0
CALLING RPC
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:19:27 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(3488,), cumulative_logprob=inf, get_num_computed_tokens=14)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.PREFILL: 1>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:19:27 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 1
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 14 15 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220, 3488]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[3488], positions=[14]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [3488]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [14]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [3488] (len: 1)
      Seq 0: input_positions in list: [14] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [3488] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 1
INFO 07-23 06:19:31 [metrics.py:489] Avg prompt throughput: 1.2 tokens/s, Avg generation throughput: 0.2 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
run_decode
1
{'1': SequenceGroup(request_id=1, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), num_seqs=1)}
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:19:31 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(), cumulative_logprob=inf, get_num_computed_tokens=13)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.DECODE: 2>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:19:31 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 50
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 13 14 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[220], positions=[13]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [220]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [13]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [220] (len: 1)
      Seq 0: input_positions in list: [13] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [220] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 0
CALLING RPC
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:19:33 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(3488,), cumulative_logprob=inf, get_num_computed_tokens=14)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.DECODE: 2>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:19:33 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 50
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 14 15 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220, 3488]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[3488], positions=[14]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [3488]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [14]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [3488] (len: 1)
      Seq 0: input_positions in list: [14] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [3488] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
REQUEST OUTPUT 2 RequestOutput(request_id=1, prompt='To be or not to be, ', prompt_token_ids=[128000, 1271, 387, 477, 539, 311, 387, 11, 220], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' question', token_ids=(3488,), cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1753251561.8060884, last_token_time=1753251573.0320914, first_scheduled_time=1753251561.8102646, first_token_time=1753251562.669284, time_in_queue=0.004176139831542969, finished_time=None, scheduler_time=0.0014872208703309298, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 0
CALLING RPC
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:19:34 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(3488, 198), cumulative_logprob=inf, get_num_computed_tokens=15)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.DECODE: 2>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:19:34 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 50
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 15 16 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220, 3488, 198]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[198], positions=[15]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [198]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [15]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [198] (len: 1)
      Seq 0: input_positions in list: [15] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [198] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
REQUEST OUTPUT 2 RequestOutput(request_id=1, prompt='To be or not to be, ', prompt_token_ids=[128000, 1271, 387, 477, 539, 311, 387, 11, 220], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' question\n', token_ids=(3488, 198), cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1753251561.8060884, last_token_time=1753251574.252483, first_scheduled_time=1753251561.8102646, first_token_time=1753251562.669284, time_in_queue=0.004176139831542969, finished_time=None, scheduler_time=0.0018085568444803357, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 0
CALLING RPC
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:19:35 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(3488, 198, 1271), cumulative_logprob=inf, get_num_computed_tokens=16)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0, 1]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.DECODE: 2>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:19:35 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 50
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 16 17 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220, 3488, 198, 1271]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[1271], positions=[16]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [1271]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [16]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [1271] (len: 1)
      Seq 0: input_positions in list: [16] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [1271] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
REQUEST OUTPUT 2 RequestOutput(request_id=1, prompt='To be or not to be, ', prompt_token_ids=[128000, 1271, 387, 477, 539, 311, 387, 11, 220], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' question\nTo', token_ids=(3488, 198, 1271), cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1753251561.8060884, last_token_time=1753251575.535937, first_scheduled_time=1753251561.8102646, first_token_time=1753251562.669284, time_in_queue=0.004176139831542969, finished_time=None, scheduler_time=0.002185076824389398, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 0
CALLING RPC
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:19:36 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(3488, 198, 1271, 387), cumulative_logprob=inf, get_num_computed_tokens=17)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0, 1]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.DECODE: 2>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:19:36 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 50
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 17 18 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220, 3488, 198, 1271, 387]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[387], positions=[17]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [387]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [17]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [387] (len: 1)
      Seq 0: input_positions in list: [17] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [387] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
REQUEST OUTPUT 2 RequestOutput(request_id=1, prompt='To be or not to be, ', prompt_token_ids=[128000, 1271, 387, 477, 539, 311, 387, 11, 220], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' question\nTo be', token_ids=(3488, 198, 1271, 387), cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1753251561.8060884, last_token_time=1753251576.8159804, first_scheduled_time=1753251561.8102646, first_token_time=1753251562.669284, time_in_queue=0.004176139831542969, finished_time=None, scheduler_time=0.0024452228099107742, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
INFO 07-23 06:19:36 [metrics.py:489] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 0
CALLING RPC
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:19:38 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(3488, 198, 1271, 387, 477), cumulative_logprob=inf, get_num_computed_tokens=18)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0, 1]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.DECODE: 2>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:19:38 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 50
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 18 19 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220, 3488, 198, 1271, 387, 477]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[477], positions=[18]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [477]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [18]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [477] (len: 1)
      Seq 0: input_positions in list: [18] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [477] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
REQUEST OUTPUT 2 RequestOutput(request_id=1, prompt='To be or not to be, ', prompt_token_ids=[128000, 1271, 387, 477, 539, 311, 387, 11, 220], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' question\nTo be or', token_ids=(3488, 198, 1271, 387, 477), cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1753251561.8060884, last_token_time=1753251578.0987399, first_scheduled_time=1753251561.8102646, first_token_time=1753251562.669284, time_in_queue=0.004176139831542969, finished_time=None, scheduler_time=0.002647457760758698, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 0
CALLING RPC
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:19:39 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(3488, 198, 1271, 387, 477, 539), cumulative_logprob=inf, get_num_computed_tokens=19)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0, 1]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.DECODE: 2>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:19:39 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 50
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 19 20 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220, 3488, 198, 1271, 387, 477, 539]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[539], positions=[19]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [539]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [19]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [539] (len: 1)
      Seq 0: input_positions in list: [19] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [539] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
REQUEST OUTPUT 2 RequestOutput(request_id=1, prompt='To be or not to be, ', prompt_token_ids=[128000, 1271, 387, 477, 539, 311, 387, 11, 220], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' question\nTo be or not', token_ids=(3488, 198, 1271, 387, 477, 539), cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1753251561.8060884, last_token_time=1753251579.379443, first_scheduled_time=1753251561.8102646, first_token_time=1753251562.669284, time_in_queue=0.004176139831542969, finished_time=None, scheduler_time=0.002895919722504914, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 0
CALLING RPC
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:19:40 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(3488, 198, 1271, 387, 477, 539, 311), cumulative_logprob=inf, get_num_computed_tokens=20)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0, 1]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.DECODE: 2>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:19:40 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 50
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 20 21 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220, 3488, 198, 1271, 387, 477, 539, 311]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[311], positions=[20]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [311]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [20]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [311] (len: 1)
      Seq 0: input_positions in list: [20] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [311] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
REQUEST OUTPUT 2 RequestOutput(request_id=1, prompt='To be or not to be, ', prompt_token_ids=[128000, 1271, 387, 477, 539, 311, 387, 11, 220], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' question\nTo be or not to', token_ids=(3488, 198, 1271, 387, 477, 539, 311), cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1753251561.8060884, last_token_time=1753251580.6586921, first_scheduled_time=1753251561.8102646, first_token_time=1753251562.669284, time_in_queue=0.004176139831542969, finished_time=None, scheduler_time=0.0031575578032061458, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 0
CALLING RPC
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:19:41 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387), cumulative_logprob=inf, get_num_computed_tokens=21)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0, 1]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.DECODE: 2>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:19:41 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 50
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 21 22 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220, 3488, 198, 1271, 387, 477, 539, 311, 387]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[387], positions=[21]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [387]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [21]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [387] (len: 1)
      Seq 0: input_positions in list: [21] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [387] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
REQUEST OUTPUT 2 RequestOutput(request_id=1, prompt='To be or not to be, ', prompt_token_ids=[128000, 1271, 387, 477, 539, 311, 387, 11, 220], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' question\nTo be or not to be', token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387), cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1753251561.8060884, last_token_time=1753251581.9433608, first_scheduled_time=1753251561.8102646, first_token_time=1753251562.669284, time_in_queue=0.004176139831542969, finished_time=None, scheduler_time=0.003360814764164388, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
INFO 07-23 06:19:41 [metrics.py:489] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 0
CALLING RPC
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:19:43 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11), cumulative_logprob=inf, get_num_computed_tokens=22)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0, 1]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.DECODE: 2>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:19:43 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 50
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 22 23 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220, 3488, 198, 1271, 387, 477, 539, 311, 387, 11]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[11], positions=[22]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [11]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [22]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [11] (len: 1)
      Seq 0: input_positions in list: [22] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [11] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
REQUEST OUTPUT 2 RequestOutput(request_id=1, prompt='To be or not to be, ', prompt_token_ids=[128000, 1271, 387, 477, 539, 311, 387, 11, 220], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' question\nTo be or not to be,', token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11), cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1753251561.8060884, last_token_time=1753251583.2295825, first_scheduled_time=1753251561.8102646, first_token_time=1753251562.669284, time_in_queue=0.004176139831542969, finished_time=None, scheduler_time=0.003559738746844232, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 0
CALLING RPC
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:19:44 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430), cumulative_logprob=inf, get_num_computed_tokens=23)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0, 1]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.DECODE: 2>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:19:44 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 50
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 23 24 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220, 3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[430], positions=[23]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [430]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [23]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [430] (len: 1)
      Seq 0: input_positions in list: [23] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [430] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
REQUEST OUTPUT 2 RequestOutput(request_id=1, prompt='To be or not to be, ', prompt_token_ids=[128000, 1271, 387, 477, 539, 311, 387, 11, 220], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' question\nTo be or not to be, that', token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430), cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1753251561.8060884, last_token_time=1753251584.51108, first_scheduled_time=1753251561.8102646, first_token_time=1753251562.669284, time_in_queue=0.004176139831542969, finished_time=None, scheduler_time=0.0037646066630259156, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 0
CALLING RPC
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:19:45 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374), cumulative_logprob=inf, get_num_computed_tokens=24)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0, 1]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.DECODE: 2>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:19:45 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 50
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 24 25 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220, 3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[374], positions=[24]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [374]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [24]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [374] (len: 1)
      Seq 0: input_positions in list: [24] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [374] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
REQUEST OUTPUT 2 RequestOutput(request_id=1, prompt='To be or not to be, ', prompt_token_ids=[128000, 1271, 387, 477, 539, 311, 387, 11, 220], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' question\nTo be or not to be, that is', token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374), cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1753251561.8060884, last_token_time=1753251585.7973773, first_scheduled_time=1753251561.8102646, first_token_time=1753251562.669284, time_in_queue=0.004176139831542969, finished_time=None, scheduler_time=0.004108190652914345, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 0
CALLING RPC
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:19:47 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279), cumulative_logprob=inf, get_num_computed_tokens=25)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0, 1]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.DECODE: 2>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:19:47 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 50
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 25 26 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220, 3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[279], positions=[25]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [279]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [25]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [279] (len: 1)
      Seq 0: input_positions in list: [25] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [279] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
REQUEST OUTPUT 2 RequestOutput(request_id=1, prompt='To be or not to be, ', prompt_token_ids=[128000, 1271, 387, 477, 539, 311, 387, 11, 220], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' question\nTo be or not to be, that is the', token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279), cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1753251561.8060884, last_token_time=1753251587.0811238, first_scheduled_time=1753251561.8102646, first_token_time=1753251562.669284, time_in_queue=0.004176139831542969, finished_time=None, scheduler_time=0.004345117602497339, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
INFO 07-23 06:19:47 [metrics.py:489] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 0
CALLING RPC
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:19:48 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488), cumulative_logprob=inf, get_num_computed_tokens=26)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0, 1]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.DECODE: 2>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:19:48 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 50
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 26 27 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220, 3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[3488], positions=[26]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [3488]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [26]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [3488] (len: 1)
      Seq 0: input_positions in list: [26] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [3488] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
REQUEST OUTPUT 2 RequestOutput(request_id=1, prompt='To be or not to be, ', prompt_token_ids=[128000, 1271, 387, 477, 539, 311, 387, 11, 220], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' question\nTo be or not to be, that is the question', token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488), cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1753251561.8060884, last_token_time=1753251588.3621256, first_scheduled_time=1753251561.8102646, first_token_time=1753251562.669284, time_in_queue=0.004176139831542969, finished_time=None, scheduler_time=0.004706702660769224, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 0
CALLING RPC
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:19:49 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198), cumulative_logprob=inf, get_num_computed_tokens=27)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0, 1]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.DECODE: 2>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:19:49 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 50
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 27 28 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220, 3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[198], positions=[27]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [198]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [27]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [198] (len: 1)
      Seq 0: input_positions in list: [27] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [198] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
REQUEST OUTPUT 2 RequestOutput(request_id=1, prompt='To be or not to be, ', prompt_token_ids=[128000, 1271, 387, 477, 539, 311, 387, 11, 220], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' question\nTo be or not to be, that is the question\n', token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198), cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1753251561.8060884, last_token_time=1753251589.6430094, first_scheduled_time=1753251561.8102646, first_token_time=1753251562.669284, time_in_queue=0.004176139831542969, finished_time=None, scheduler_time=0.005028309766203165, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 0
CALLING RPC
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:19:50 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729), cumulative_logprob=inf, get_num_computed_tokens=28)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0, 1]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.DECODE: 2>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:19:50 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 50
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 28 29 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220, 3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[25729], positions=[28]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [25729]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [28]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [25729] (len: 1)
      Seq 0: input_positions in list: [28] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [25729] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
REQUEST OUTPUT 2 RequestOutput(request_id=1, prompt='To be or not to be, ', prompt_token_ids=[128000, 1271, 387, 477, 539, 311, 387, 11, 220], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' question\nTo be or not to be, that is the question\nWhether', token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729), cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1753251561.8060884, last_token_time=1753251590.9267008, first_scheduled_time=1753251561.8102646, first_token_time=1753251562.669284, time_in_queue=0.004176139831542969, finished_time=None, scheduler_time=0.0052517198491841555, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 0
CALLING RPC
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:19:52 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364), cumulative_logprob=inf, get_num_computed_tokens=29)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0, 1]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.DECODE: 2>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:19:52 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 50
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 29 30 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220, 3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[364], positions=[29]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [364]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [29]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [364] (len: 1)
      Seq 0: input_positions in list: [29] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [364] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
REQUEST OUTPUT 2 RequestOutput(request_id=1, prompt='To be or not to be, ', prompt_token_ids=[128000, 1271, 387, 477, 539, 311, 387, 11, 220], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=" question\nTo be or not to be, that is the question\nWhether '", token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364), cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1753251561.8060884, last_token_time=1753251592.208295, first_scheduled_time=1753251561.8102646, first_token_time=1753251562.669284, time_in_queue=0.004176139831542969, finished_time=None, scheduler_time=0.005501736770384014, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
INFO 07-23 06:19:52 [metrics.py:489] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 0
CALLING RPC
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:19:53 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83), cumulative_logprob=inf, get_num_computed_tokens=30)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0, 1]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.DECODE: 2>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:19:53 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 50
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 30 31 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220, 3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[83], positions=[30]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [83]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [30]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [83] (len: 1)
      Seq 0: input_positions in list: [30] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [83] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
REQUEST OUTPUT 2 RequestOutput(request_id=1, prompt='To be or not to be, ', prompt_token_ids=[128000, 1271, 387, 477, 539, 311, 387, 11, 220], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=" question\nTo be or not to be, that is the question\nWhether 't", token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83), cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1753251561.8060884, last_token_time=1753251593.4933865, first_scheduled_time=1753251561.8102646, first_token_time=1753251562.669284, time_in_queue=0.004176139831542969, finished_time=None, scheduler_time=0.005704751703888178, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 0
CALLING RPC
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:19:54 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285), cumulative_logprob=inf, get_num_computed_tokens=31)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0, 1]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.DECODE: 2>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:19:54 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 50
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 31 32 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220, 3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[285], positions=[31]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [285]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [31]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [285] (len: 1)
      Seq 0: input_positions in list: [31] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [285] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
REQUEST OUTPUT 2 RequestOutput(request_id=1, prompt='To be or not to be, ', prompt_token_ids=[128000, 1271, 387, 477, 539, 311, 387, 11, 220], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=" question\nTo be or not to be, that is the question\nWhether 'tis", token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285), cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1753251561.8060884, last_token_time=1753251594.775514, first_scheduled_time=1753251561.8102646, first_token_time=1753251562.669284, time_in_queue=0.004176139831542969, finished_time=None, scheduler_time=0.005995114683173597, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 0
CALLING RPC
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:19:56 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348), cumulative_logprob=inf, get_num_computed_tokens=32)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0, 1, 2]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.DECODE: 2>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:19:56 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 50
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 32 33 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220, 3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[13348], positions=[32]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [13348]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [32]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [13348] (len: 1)
      Seq 0: input_positions in list: [32] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [13348] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
REQUEST OUTPUT 2 RequestOutput(request_id=1, prompt='To be or not to be, ', prompt_token_ids=[128000, 1271, 387, 477, 539, 311, 387, 11, 220], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=" question\nTo be or not to be, that is the question\nWhether 'tis nob", token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348), cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1753251561.8060884, last_token_time=1753251596.053421, first_scheduled_time=1753251561.8102646, first_token_time=1753251562.669284, time_in_queue=0.004176139831542969, finished_time=None, scheduler_time=0.006246124743483961, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 0
CALLING RPC
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:19:57 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565), cumulative_logprob=inf, get_num_computed_tokens=33)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0, 1, 2]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.DECODE: 2>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:19:57 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 50
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 33 34 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220, 3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[1565], positions=[33]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [1565]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [33]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [1565] (len: 1)
      Seq 0: input_positions in list: [33] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [1565] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
REQUEST OUTPUT 2 RequestOutput(request_id=1, prompt='To be or not to be, ', prompt_token_ids=[128000, 1271, 387, 477, 539, 311, 387, 11, 220], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=" question\nTo be or not to be, that is the question\nWhether 'tis nobler", token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565), cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1753251561.8060884, last_token_time=1753251597.3310597, first_scheduled_time=1753251561.8102646, first_token_time=1753251562.669284, time_in_queue=0.004176139831542969, finished_time=None, scheduler_time=0.006480298819951713, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
INFO 07-23 06:19:57 [metrics.py:489] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 0
CALLING RPC
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:19:58 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304), cumulative_logprob=inf, get_num_computed_tokens=34)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0, 1, 2]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.DECODE: 2>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:19:58 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 50
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 34 35 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220, 3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[304], positions=[34]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [304]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [34]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [304] (len: 1)
      Seq 0: input_positions in list: [34] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [304] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
REQUEST OUTPUT 2 RequestOutput(request_id=1, prompt='To be or not to be, ', prompt_token_ids=[128000, 1271, 387, 477, 539, 311, 387, 11, 220], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=" question\nTo be or not to be, that is the question\nWhether 'tis nobler in", token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304), cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1753251561.8060884, last_token_time=1753251598.609404, first_scheduled_time=1753251561.8102646, first_token_time=1753251562.669284, time_in_queue=0.004176139831542969, finished_time=None, scheduler_time=0.006694830837659538, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 0
CALLING RPC
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:19:59 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279), cumulative_logprob=inf, get_num_computed_tokens=35)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0, 1, 2]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.DECODE: 2>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:19:59 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 50
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 35 36 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220, 3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[279], positions=[35]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [279]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [35]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [279] (len: 1)
      Seq 0: input_positions in list: [35] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [279] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
REQUEST OUTPUT 2 RequestOutput(request_id=1, prompt='To be or not to be, ', prompt_token_ids=[128000, 1271, 387, 477, 539, 311, 387, 11, 220], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=" question\nTo be or not to be, that is the question\nWhether 'tis nobler in the", token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279), cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1753251561.8060884, last_token_time=1753251599.8930743, first_scheduled_time=1753251561.8102646, first_token_time=1753251562.669284, time_in_queue=0.004176139831542969, finished_time=None, scheduler_time=0.006929671857506037, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 0
CALLING RPC
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:20:01 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059), cumulative_logprob=inf, get_num_computed_tokens=36)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0, 1, 2]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.DECODE: 2>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:20:01 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 50
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 36 37 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220, 3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[4059], positions=[36]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [4059]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [36]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [4059] (len: 1)
      Seq 0: input_positions in list: [36] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [4059] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
REQUEST OUTPUT 2 RequestOutput(request_id=1, prompt='To be or not to be, ', prompt_token_ids=[128000, 1271, 387, 477, 539, 311, 387, 11, 220], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=" question\nTo be or not to be, that is the question\nWhether 'tis nobler in the mind", token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059), cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1753251561.8060884, last_token_time=1753251601.1787372, first_scheduled_time=1753251561.8102646, first_token_time=1753251562.669284, time_in_queue=0.004176139831542969, finished_time=None, scheduler_time=0.00728731881827116, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 0
CALLING RPC
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:20:02 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311), cumulative_logprob=inf, get_num_computed_tokens=37)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0, 1, 2]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.DECODE: 2>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:20:02 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 50
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 37 38 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220, 3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[311], positions=[37]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [311]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [37]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [311] (len: 1)
      Seq 0: input_positions in list: [37] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [311] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
REQUEST OUTPUT 2 RequestOutput(request_id=1, prompt='To be or not to be, ', prompt_token_ids=[128000, 1271, 387, 477, 539, 311, 387, 11, 220], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=" question\nTo be or not to be, that is the question\nWhether 'tis nobler in the mind to", token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311), cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1753251561.8060884, last_token_time=1753251602.4608257, first_scheduled_time=1753251561.8102646, first_token_time=1753251562.669284, time_in_queue=0.004176139831542969, finished_time=None, scheduler_time=0.00756723084487021, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
INFO 07-23 06:20:02 [metrics.py:489] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 0
CALLING RPC
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:20:03 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831), cumulative_logprob=inf, get_num_computed_tokens=38)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0, 1, 2]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.DECODE: 2>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:20:03 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 50
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 38 39 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220, 3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[7831], positions=[38]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [7831]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [38]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [7831] (len: 1)
      Seq 0: input_positions in list: [38] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [7831] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
REQUEST OUTPUT 2 RequestOutput(request_id=1, prompt='To be or not to be, ', prompt_token_ids=[128000, 1271, 387, 477, 539, 311, 387, 11, 220], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=" question\nTo be or not to be, that is the question\nWhether 'tis nobler in the mind to suffer", token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831), cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1753251561.8060884, last_token_time=1753251603.7430177, first_scheduled_time=1753251561.8102646, first_token_time=1753251562.669284, time_in_queue=0.004176139831542969, finished_time=None, scheduler_time=0.007961698807775974, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 0
CALLING RPC
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:20:05 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198), cumulative_logprob=inf, get_num_computed_tokens=39)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0, 1, 2]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.DECODE: 2>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:20:05 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 50
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 39 40 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220, 3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[198], positions=[39]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [198]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [39]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [198] (len: 1)
      Seq 0: input_positions in list: [39] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [198] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
REQUEST OUTPUT 2 RequestOutput(request_id=1, prompt='To be or not to be, ', prompt_token_ids=[128000, 1271, 387, 477, 539, 311, 387, 11, 220], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=" question\nTo be or not to be, that is the question\nWhether 'tis nobler in the mind to suffer\n", token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198), cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1753251561.8060884, last_token_time=1753251605.0235934, first_scheduled_time=1753251561.8102646, first_token_time=1753251562.669284, time_in_queue=0.004176139831542969, finished_time=None, scheduler_time=0.008206591825000942, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 0
CALLING RPC
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:20:06 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791), cumulative_logprob=inf, get_num_computed_tokens=40)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0, 1, 2]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.DECODE: 2>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:20:06 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 50
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 40 41 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220, 3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[791], positions=[40]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [791]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [40]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [791] (len: 1)
      Seq 0: input_positions in list: [40] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [791] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
REQUEST OUTPUT 2 RequestOutput(request_id=1, prompt='To be or not to be, ', prompt_token_ids=[128000, 1271, 387, 477, 539, 311, 387, 11, 220], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=" question\nTo be or not to be, that is the question\nWhether 'tis nobler in the mind to suffer\nThe", token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791), cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1753251561.8060884, last_token_time=1753251606.3062918, first_scheduled_time=1753251561.8102646, first_token_time=1753251562.669284, time_in_queue=0.004176139831542969, finished_time=None, scheduler_time=0.008595914812758565, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 0
CALLING RPC
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:20:07 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776), cumulative_logprob=inf, get_num_computed_tokens=41)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0, 1, 2]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.DECODE: 2>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:20:07 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 50
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 41 42 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220, 3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[1776], positions=[41]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [1776]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [41]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [1776] (len: 1)
      Seq 0: input_positions in list: [41] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [1776] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
REQUEST OUTPUT 2 RequestOutput(request_id=1, prompt='To be or not to be, ', prompt_token_ids=[128000, 1271, 387, 477, 539, 311, 387, 11, 220], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=" question\nTo be or not to be, that is the question\nWhether 'tis nobler in the mind to suffer\nThe sl", token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776), cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1753251561.8060884, last_token_time=1753251607.5905218, first_scheduled_time=1753251561.8102646, first_token_time=1753251562.669284, time_in_queue=0.004176139831542969, finished_time=None, scheduler_time=0.008888506796211004, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
INFO 07-23 06:20:07 [metrics.py:489] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 0
CALLING RPC
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:20:08 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826), cumulative_logprob=inf, get_num_computed_tokens=42)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0, 1, 2]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.DECODE: 2>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:20:08 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 50
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 42 43 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220, 3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[826], positions=[42]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [826]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [42]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [826] (len: 1)
      Seq 0: input_positions in list: [42] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [826] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
REQUEST OUTPUT 2 RequestOutput(request_id=1, prompt='To be or not to be, ', prompt_token_ids=[128000, 1271, 387, 477, 539, 311, 387, 11, 220], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=" question\nTo be or not to be, that is the question\nWhether 'tis nobler in the mind to suffer\nThe slings", token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826), cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1753251561.8060884, last_token_time=1753251608.8785448, first_scheduled_time=1753251561.8102646, first_token_time=1753251562.669284, time_in_queue=0.004176139831542969, finished_time=None, scheduler_time=0.009202296729199588, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 0
CALLING RPC
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:20:10 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323), cumulative_logprob=inf, get_num_computed_tokens=43)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0, 1, 2]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.DECODE: 2>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:20:10 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 50
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 43 44 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220, 3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[323], positions=[43]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [323]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [43]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [323] (len: 1)
      Seq 0: input_positions in list: [43] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [323] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
REQUEST OUTPUT 2 RequestOutput(request_id=1, prompt='To be or not to be, ', prompt_token_ids=[128000, 1271, 387, 477, 539, 311, 387, 11, 220], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=" question\nTo be or not to be, that is the question\nWhether 'tis nobler in the mind to suffer\nThe slings and", token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323), cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1753251561.8060884, last_token_time=1753251610.1633577, first_scheduled_time=1753251561.8102646, first_token_time=1753251562.669284, time_in_queue=0.004176139831542969, finished_time=None, scheduler_time=0.00944718369282782, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 0
CALLING RPC
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:20:11 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057), cumulative_logprob=inf, get_num_computed_tokens=44)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0, 1, 2]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.DECODE: 2>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:20:11 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 50
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 44 45 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220, 3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[38057], positions=[44]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [38057]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [44]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [38057] (len: 1)
      Seq 0: input_positions in list: [44] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [38057] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
REQUEST OUTPUT 2 RequestOutput(request_id=1, prompt='To be or not to be, ', prompt_token_ids=[128000, 1271, 387, 477, 539, 311, 387, 11, 220], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=" question\nTo be or not to be, that is the question\nWhether 'tis nobler in the mind to suffer\nThe slings and arrows", token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057), cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1753251561.8060884, last_token_time=1753251611.4458168, first_scheduled_time=1753251561.8102646, first_token_time=1753251562.669284, time_in_queue=0.004176139831542969, finished_time=None, scheduler_time=0.009801200707443058, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 0
CALLING RPC
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:20:12 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315), cumulative_logprob=inf, get_num_computed_tokens=45)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0, 1, 2]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.DECODE: 2>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:20:12 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 50
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 45 46 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220, 3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[315], positions=[45]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [315]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [45]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [315] (len: 1)
      Seq 0: input_positions in list: [45] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [315] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
REQUEST OUTPUT 2 RequestOutput(request_id=1, prompt='To be or not to be, ', prompt_token_ids=[128000, 1271, 387, 477, 539, 311, 387, 11, 220], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=" question\nTo be or not to be, that is the question\nWhether 'tis nobler in the mind to suffer\nThe slings and arrows of", token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315), cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1753251561.8060884, last_token_time=1753251612.7297757, first_scheduled_time=1753251561.8102646, first_token_time=1753251562.669284, time_in_queue=0.004176139831542969, finished_time=None, scheduler_time=0.010072351666167378, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
INFO 07-23 06:20:12 [metrics.py:489] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 0
CALLING RPC
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:20:14 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588), cumulative_logprob=inf, get_num_computed_tokens=46)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0, 1, 2]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.DECODE: 2>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:20:14 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 50
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 46 47 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220, 3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[55588], positions=[46]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [55588]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [46]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [55588] (len: 1)
      Seq 0: input_positions in list: [46] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [55588] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
REQUEST OUTPUT 2 RequestOutput(request_id=1, prompt='To be or not to be, ', prompt_token_ids=[128000, 1271, 387, 477, 539, 311, 387, 11, 220], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=" question\nTo be or not to be, that is the question\nWhether 'tis nobler in the mind to suffer\nThe slings and arrows of outrageous", token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588), cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1753251561.8060884, last_token_time=1753251614.0150206, first_scheduled_time=1753251561.8102646, first_token_time=1753251562.669284, time_in_queue=0.004176139831542969, finished_time=None, scheduler_time=0.010381647734902799, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 0
CALLING RPC
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:20:15 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588, 33415), cumulative_logprob=inf, get_num_computed_tokens=47)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0, 1, 2]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.DECODE: 2>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:20:15 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 50
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 47 48 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220, 3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588, 33415]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[33415], positions=[47]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [33415]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [47]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [33415] (len: 1)
      Seq 0: input_positions in list: [47] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [33415] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
REQUEST OUTPUT 2 RequestOutput(request_id=1, prompt='To be or not to be, ', prompt_token_ids=[128000, 1271, 387, 477, 539, 311, 387, 11, 220], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=" question\nTo be or not to be, that is the question\nWhether 'tis nobler in the mind to suffer\nThe slings and arrows of outrageous fortune", token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588, 33415), cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1753251561.8060884, last_token_time=1753251615.2935982, first_scheduled_time=1753251561.8102646, first_token_time=1753251562.669284, time_in_queue=0.004176139831542969, finished_time=None, scheduler_time=0.010686590685509145, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 0
CALLING RPC
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:20:16 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588, 33415, 198), cumulative_logprob=inf, get_num_computed_tokens=48)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0, 1, 2, 3]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.DECODE: 2>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:20:16 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 50
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 48 49 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220, 3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588, 33415, 198]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[198], positions=[48]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [198]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [48]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [198] (len: 1)
      Seq 0: input_positions in list: [48] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [198] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
REQUEST OUTPUT 2 RequestOutput(request_id=1, prompt='To be or not to be, ', prompt_token_ids=[128000, 1271, 387, 477, 539, 311, 387, 11, 220], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=" question\nTo be or not to be, that is the question\nWhether 'tis nobler in the mind to suffer\nThe slings and arrows of outrageous fortune\n", token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588, 33415, 198), cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1753251561.8060884, last_token_time=1753251616.5807476, first_scheduled_time=1753251561.8102646, first_token_time=1753251562.669284, time_in_queue=0.004176139831542969, finished_time=None, scheduler_time=0.01099503762088716, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 0
CALLING RPC
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:20:17 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588, 33415, 198, 2244), cumulative_logprob=inf, get_num_computed_tokens=49)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0, 1, 2, 3]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.DECODE: 2>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:20:17 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 50
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 49 50 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220, 3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588, 33415, 198, 2244]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[2244], positions=[49]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [2244]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [49]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [2244] (len: 1)
      Seq 0: input_positions in list: [49] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [2244] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
REQUEST OUTPUT 2 RequestOutput(request_id=1, prompt='To be or not to be, ', prompt_token_ids=[128000, 1271, 387, 477, 539, 311, 387, 11, 220], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=" question\nTo be or not to be, that is the question\nWhether 'tis nobler in the mind to suffer\nThe slings and arrows of outrageous fortune\nOr", token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588, 33415, 198, 2244), cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1753251561.8060884, last_token_time=1753251617.8597856, first_scheduled_time=1753251561.8102646, first_token_time=1753251562.669284, time_in_queue=0.004176139831542969, finished_time=None, scheduler_time=0.011252116644755006, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
INFO 07-23 06:20:17 [metrics.py:489] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 0
CALLING RPC
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:20:19 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588, 33415, 198, 2244, 311), cumulative_logprob=inf, get_num_computed_tokens=50)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0, 1, 2, 3]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.DECODE: 2>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:20:19 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 50
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 50 51 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220, 3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588, 33415, 198, 2244, 311]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[311], positions=[50]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [311]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [50]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [311] (len: 1)
      Seq 0: input_positions in list: [50] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [311] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
REQUEST OUTPUT 2 RequestOutput(request_id=1, prompt='To be or not to be, ', prompt_token_ids=[128000, 1271, 387, 477, 539, 311, 387, 11, 220], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=" question\nTo be or not to be, that is the question\nWhether 'tis nobler in the mind to suffer\nThe slings and arrows of outrageous fortune\nOr to", token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588, 33415, 198, 2244, 311), cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1753251561.8060884, last_token_time=1753251619.137905, first_scheduled_time=1753251561.8102646, first_token_time=1753251562.669284, time_in_queue=0.004176139831542969, finished_time=None, scheduler_time=0.0115313776768744, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 0
CALLING RPC
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:20:20 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588, 33415, 198, 2244, 311, 1935), cumulative_logprob=inf, get_num_computed_tokens=51)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0, 1, 2, 3]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.DECODE: 2>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:20:20 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 50
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 51 52 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220, 3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588, 33415, 198, 2244, 311, 1935]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[1935], positions=[51]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [1935]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [51]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [1935] (len: 1)
      Seq 0: input_positions in list: [51] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [1935] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
REQUEST OUTPUT 2 RequestOutput(request_id=1, prompt='To be or not to be, ', prompt_token_ids=[128000, 1271, 387, 477, 539, 311, 387, 11, 220], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=" question\nTo be or not to be, that is the question\nWhether 'tis nobler in the mind to suffer\nThe slings and arrows of outrageous fortune\nOr to take", token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588, 33415, 198, 2244, 311, 1935), cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1753251561.8060884, last_token_time=1753251620.423434, first_scheduled_time=1753251561.8102646, first_token_time=1753251562.669284, time_in_queue=0.004176139831542969, finished_time=None, scheduler_time=0.011944076744839549, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 0
CALLING RPC
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:20:21 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588, 33415, 198, 2244, 311, 1935, 11977), cumulative_logprob=inf, get_num_computed_tokens=52)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0, 1, 2, 3]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.DECODE: 2>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:20:21 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 50
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 52 53 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220, 3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588, 33415, 198, 2244, 311, 1935, 11977]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[11977], positions=[52]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [11977]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [52]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [11977] (len: 1)
      Seq 0: input_positions in list: [52] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [11977] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
REQUEST OUTPUT 2 RequestOutput(request_id=1, prompt='To be or not to be, ', prompt_token_ids=[128000, 1271, 387, 477, 539, 311, 387, 11, 220], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=" question\nTo be or not to be, that is the question\nWhether 'tis nobler in the mind to suffer\nThe slings and arrows of outrageous fortune\nOr to take arms", token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588, 33415, 198, 2244, 311, 1935, 11977), cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1753251561.8060884, last_token_time=1753251621.704, first_scheduled_time=1753251561.8102646, first_token_time=1753251562.669284, time_in_queue=0.004176139831542969, finished_time=None, scheduler_time=0.01222532382234931, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 0
CALLING RPC
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:20:22 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588, 33415, 198, 2244, 311, 1935, 11977, 2403), cumulative_logprob=inf, get_num_computed_tokens=53)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0, 1, 2, 3]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.DECODE: 2>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:20:22 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 50
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 53 54 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220, 3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588, 33415, 198, 2244, 311, 1935, 11977, 2403]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[2403], positions=[53]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [2403]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [53]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [2403] (len: 1)
      Seq 0: input_positions in list: [53] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [2403] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
REQUEST OUTPUT 2 RequestOutput(request_id=1, prompt='To be or not to be, ', prompt_token_ids=[128000, 1271, 387, 477, 539, 311, 387, 11, 220], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=" question\nTo be or not to be, that is the question\nWhether 'tis nobler in the mind to suffer\nThe slings and arrows of outrageous fortune\nOr to take arms against", token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588, 33415, 198, 2244, 311, 1935, 11977, 2403), cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1753251561.8060884, last_token_time=1753251622.9851143, first_scheduled_time=1753251561.8102646, first_token_time=1753251562.669284, time_in_queue=0.004176139831542969, finished_time=None, scheduler_time=0.012650954769924283, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
INFO 07-23 06:20:22 [metrics.py:489] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 0
CALLING RPC
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:20:24 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588, 33415, 198, 2244, 311, 1935, 11977, 2403, 264), cumulative_logprob=inf, get_num_computed_tokens=54)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0, 1, 2, 3]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.DECODE: 2>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:20:24 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 50
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 54 55 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220, 3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588, 33415, 198, 2244, 311, 1935, 11977, 2403, 264]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[264], positions=[54]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [264]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [54]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [264] (len: 1)
      Seq 0: input_positions in list: [54] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [264] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
REQUEST OUTPUT 2 RequestOutput(request_id=1, prompt='To be or not to be, ', prompt_token_ids=[128000, 1271, 387, 477, 539, 311, 387, 11, 220], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=" question\nTo be or not to be, that is the question\nWhether 'tis nobler in the mind to suffer\nThe slings and arrows of outrageous fortune\nOr to take arms against a", token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588, 33415, 198, 2244, 311, 1935, 11977, 2403, 264), cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1753251561.8060884, last_token_time=1753251624.2736325, first_scheduled_time=1753251561.8102646, first_token_time=1753251562.669284, time_in_queue=0.004176139831542969, finished_time=None, scheduler_time=0.012942935805767775, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 0
CALLING RPC
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:20:25 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588, 33415, 198, 2244, 311, 1935, 11977, 2403, 264, 9581), cumulative_logprob=inf, get_num_computed_tokens=55)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0, 1, 2, 3]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.DECODE: 2>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:20:25 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 50
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 55 56 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220, 3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588, 33415, 198, 2244, 311, 1935, 11977, 2403, 264, 9581]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[9581], positions=[55]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [9581]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [55]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [9581] (len: 1)
      Seq 0: input_positions in list: [55] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [9581] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
REQUEST OUTPUT 2 RequestOutput(request_id=1, prompt='To be or not to be, ', prompt_token_ids=[128000, 1271, 387, 477, 539, 311, 387, 11, 220], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=" question\nTo be or not to be, that is the question\nWhether 'tis nobler in the mind to suffer\nThe slings and arrows of outrageous fortune\nOr to take arms against a sea", token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588, 33415, 198, 2244, 311, 1935, 11977, 2403, 264, 9581), cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1753251561.8060884, last_token_time=1753251625.5560546, first_scheduled_time=1753251561.8102646, first_token_time=1753251562.669284, time_in_queue=0.004176139831542969, finished_time=None, scheduler_time=0.013294416829012334, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 0
CALLING RPC
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:20:26 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588, 33415, 198, 2244, 311, 1935, 11977, 2403, 264, 9581, 315), cumulative_logprob=inf, get_num_computed_tokens=56)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0, 1, 2, 3]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.DECODE: 2>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:20:26 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 50
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 56 57 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220, 3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588, 33415, 198, 2244, 311, 1935, 11977, 2403, 264, 9581, 315]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[315], positions=[56]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [315]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [56]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [315] (len: 1)
      Seq 0: input_positions in list: [56] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [315] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
REQUEST OUTPUT 2 RequestOutput(request_id=1, prompt='To be or not to be, ', prompt_token_ids=[128000, 1271, 387, 477, 539, 311, 387, 11, 220], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=" question\nTo be or not to be, that is the question\nWhether 'tis nobler in the mind to suffer\nThe slings and arrows of outrageous fortune\nOr to take arms against a sea of", token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588, 33415, 198, 2244, 311, 1935, 11977, 2403, 264, 9581, 315), cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1753251561.8060884, last_token_time=1753251626.8345513, first_scheduled_time=1753251561.8102646, first_token_time=1753251562.669284, time_in_queue=0.004176139831542969, finished_time=None, scheduler_time=0.013552228920161724, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 0
CALLING RPC
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:20:28 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588, 33415, 198, 2244, 311, 1935, 11977, 2403, 264, 9581, 315, 35665), cumulative_logprob=inf, get_num_computed_tokens=57)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0, 1, 2, 3]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.DECODE: 2>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:20:28 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 50
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 57 58 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220, 3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588, 33415, 198, 2244, 311, 1935, 11977, 2403, 264, 9581, 315, 35665]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[35665], positions=[57]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [35665]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [57]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [35665] (len: 1)
      Seq 0: input_positions in list: [57] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [35665] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
REQUEST OUTPUT 2 RequestOutput(request_id=1, prompt='To be or not to be, ', prompt_token_ids=[128000, 1271, 387, 477, 539, 311, 387, 11, 220], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=" question\nTo be or not to be, that is the question\nWhether 'tis nobler in the mind to suffer\nThe slings and arrows of outrageous fortune\nOr to take arms against a sea of troubles", token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588, 33415, 198, 2244, 311, 1935, 11977, 2403, 264, 9581, 315, 35665), cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1753251561.8060884, last_token_time=1753251628.1186209, first_scheduled_time=1753251561.8102646, first_token_time=1753251562.669284, time_in_queue=0.004176139831542969, finished_time=None, scheduler_time=0.01388434402178973, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
INFO 07-23 06:20:28 [metrics.py:489] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 0
CALLING RPC
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:20:29 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588, 33415, 198, 2244, 311, 1935, 11977, 2403, 264, 9581, 315, 35665, 198), cumulative_logprob=inf, get_num_computed_tokens=58)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0, 1, 2, 3]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.DECODE: 2>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:20:29 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 50
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 58 59 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220, 3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588, 33415, 198, 2244, 311, 1935, 11977, 2403, 264, 9581, 315, 35665, 198]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[198], positions=[58]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [198]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [58]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [198] (len: 1)
      Seq 0: input_positions in list: [58] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [198] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
REQUEST OUTPUT 2 RequestOutput(request_id=1, prompt='To be or not to be, ', prompt_token_ids=[128000, 1271, 387, 477, 539, 311, 387, 11, 220], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=" question\nTo be or not to be, that is the question\nWhether 'tis nobler in the mind to suffer\nThe slings and arrows of outrageous fortune\nOr to take arms against a sea of troubles\n", token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588, 33415, 198, 2244, 311, 1935, 11977, 2403, 264, 9581, 315, 35665, 198), cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1753251561.8060884, last_token_time=1753251629.4042265, first_scheduled_time=1753251561.8102646, first_token_time=1753251562.669284, time_in_queue=0.004176139831542969, finished_time=None, scheduler_time=0.014173520030453801, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 0
CALLING RPC
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:20:30 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588, 33415, 198, 2244, 311, 1935, 11977, 2403, 264, 9581, 315, 35665, 198, 3112), cumulative_logprob=inf, get_num_computed_tokens=59)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0, 1, 2, 3]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.DECODE: 2>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:20:30 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 50
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 59 60 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220, 3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588, 33415, 198, 2244, 311, 1935, 11977, 2403, 264, 9581, 315, 35665, 198, 3112]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[3112], positions=[59]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [3112]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [59]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [3112] (len: 1)
      Seq 0: input_positions in list: [59] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [3112] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
REQUEST OUTPUT 2 RequestOutput(request_id=1, prompt='To be or not to be, ', prompt_token_ids=[128000, 1271, 387, 477, 539, 311, 387, 11, 220], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=" question\nTo be or not to be, that is the question\nWhether 'tis nobler in the mind to suffer\nThe slings and arrows of outrageous fortune\nOr to take arms against a sea of troubles\nAnd", token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588, 33415, 198, 2244, 311, 1935, 11977, 2403, 264, 9581, 315, 35665, 198, 3112), cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1753251561.8060884, last_token_time=1753251630.6874166, first_scheduled_time=1753251561.8102646, first_token_time=1753251562.669284, time_in_queue=0.004176139831542969, finished_time=None, scheduler_time=0.01454760495107621, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 0
CALLING RPC
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:20:32 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588, 33415, 198, 2244, 311, 1935, 11977, 2403, 264, 9581, 315, 35665, 198, 3112, 555), cumulative_logprob=inf, get_num_computed_tokens=60)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0, 1, 2, 3]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.DECODE: 2>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:20:32 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 50
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 60 61 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220, 3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588, 33415, 198, 2244, 311, 1935, 11977, 2403, 264, 9581, 315, 35665, 198, 3112, 555]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[555], positions=[60]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [555]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [60]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [555] (len: 1)
      Seq 0: input_positions in list: [60] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [555] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
REQUEST OUTPUT 2 RequestOutput(request_id=1, prompt='To be or not to be, ', prompt_token_ids=[128000, 1271, 387, 477, 539, 311, 387, 11, 220], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=" question\nTo be or not to be, that is the question\nWhether 'tis nobler in the mind to suffer\nThe slings and arrows of outrageous fortune\nOr to take arms against a sea of troubles\nAnd by", token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588, 33415, 198, 2244, 311, 1935, 11977, 2403, 264, 9581, 315, 35665, 198, 3112, 555), cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1753251561.8060884, last_token_time=1753251632.0247073, first_scheduled_time=1753251561.8102646, first_token_time=1753251562.669284, time_in_queue=0.004176139831542969, finished_time=None, scheduler_time=0.01484530488960445, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 0
CALLING RPC
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:20:33 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588, 33415, 198, 2244, 311, 1935, 11977, 2403, 264, 9581, 315, 35665, 198, 3112, 555, 31322), cumulative_logprob=inf, get_num_computed_tokens=61)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0, 1, 2, 3]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.DECODE: 2>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:20:33 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 50
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 61 62 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220, 3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588, 33415, 198, 2244, 311, 1935, 11977, 2403, 264, 9581, 315, 35665, 198, 3112, 555, 31322]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[31322], positions=[61]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [31322]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [61]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [31322] (len: 1)
      Seq 0: input_positions in list: [61] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [31322] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
REQUEST OUTPUT 2 RequestOutput(request_id=1, prompt='To be or not to be, ', prompt_token_ids=[128000, 1271, 387, 477, 539, 311, 387, 11, 220], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=" question\nTo be or not to be, that is the question\nWhether 'tis nobler in the mind to suffer\nThe slings and arrows of outrageous fortune\nOr to take arms against a sea of troubles\nAnd by opposing", token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588, 33415, 198, 2244, 311, 1935, 11977, 2403, 264, 9581, 315, 35665, 198, 3112, 555, 31322), cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1753251561.8060884, last_token_time=1753251633.3061519, first_scheduled_time=1753251561.8102646, first_token_time=1753251562.669284, time_in_queue=0.004176139831542969, finished_time=None, scheduler_time=0.015105839935131371, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
INFO 07-23 06:20:33 [metrics.py:489] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.8 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 0
CALLING RPC
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:20:34 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588, 33415, 198, 2244, 311, 1935, 11977, 2403, 264, 9581, 315, 35665, 198, 3112, 555, 31322, 842), cumulative_logprob=inf, get_num_computed_tokens=62)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0, 1, 2, 3]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.DECODE: 2>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:20:34 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 50
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 62 63 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220, 3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588, 33415, 198, 2244, 311, 1935, 11977, 2403, 264, 9581, 315, 35665, 198, 3112, 555, 31322, 842]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[842], positions=[62]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [842]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [62]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [842] (len: 1)
      Seq 0: input_positions in list: [62] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [842] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
REQUEST OUTPUT 2 RequestOutput(request_id=1, prompt='To be or not to be, ', prompt_token_ids=[128000, 1271, 387, 477, 539, 311, 387, 11, 220], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=" question\nTo be or not to be, that is the question\nWhether 'tis nobler in the mind to suffer\nThe slings and arrows of outrageous fortune\nOr to take arms against a sea of troubles\nAnd by opposing end", token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588, 33415, 198, 2244, 311, 1935, 11977, 2403, 264, 9581, 315, 35665, 198, 3112, 555, 31322, 842), cumulative_logprob=None, logprobs=None, finish_reason=None, stop_reason=None)], finished=False, metrics=RequestMetrics(arrival_time=1753251561.8060884, last_token_time=1753251634.5852997, first_scheduled_time=1753251561.8102646, first_token_time=1753251562.669284, time_in_queue=0.004176139831542969, finished_time=None, scheduler_time=0.015388228930532932, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
UPDATE_NUM_COMPUTED_TOKENS SequenceStage.DECODE 0
CALLING RPC
APPENDED NEW TOKENS array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
(128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SENDING FULL METADATA (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
INFO 07-23 06:20:35 [model_runner.py:1281] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220]), output_token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588, 33415, 198, 2244, 311, 1935, 11977, 2403, 264, 9581, 315, 35665, 198, 3112, 555, 31322, 842, 1124), cumulative_logprob=inf, get_num_computed_tokens=63)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=42, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=50, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0, 1, 2, 3]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.DECODE: 2>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 07-23 06:20:35 [model_runner.py:1284] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 50
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 63 64 [128000, 1271, 387, 477, 539, 311, 387, 11, 220, 128000, 9210, 374, 279, 220, 3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588, 33415, 198, 2244, 311, 1935, 11977, 2403, 264, 9581, 315, 35665, 198, 3112, 555, 31322, 842, 1124]
DEBUG_MODEL_RUNNER: _compute_lens preparing tokens=[1124], positions=[63]
added seq group 1
builder is building
DEBUG_MODEL_RUNNER: build() - 1. Successfully flattened Python lists.
DEBUG_MODEL_RUNNER: build() - 1a. input_tokens (list): [1124]... (len: 1)
DEBUG_MODEL_RUNNER: build() - 1b. input_positions (list): [63]... (len: 1)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [1124] (len: 1)
      Seq 0: input_positions in list: [63] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [1124] (len: 1)
DEBUG_MODEL_RUNNER: build() - 2. Successfully created input_tokens_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 3. Successfully created input_positions_tensor. Shape: torch.Size([1])
DEBUG_MODEL_RUNNER: build() - 4. Successfully built attention metadata.
DEBUG_MODEL_RUNNER: build() - 5. Finalizing ModelInputForGPU object.
FREEING SEQ 0
REQUEST OUTPUT 1 RequestOutput(request_id=1, prompt='To be or not to be, ', prompt_token_ids=[128000, 1271, 387, 477, 539, 311, 387, 11, 220], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=" question\nTo be or not to be, that is the question\nWhether 'tis nobler in the mind to suffer\nThe slings and arrows of outrageous fortune\nOr to take arms against a sea of troubles\nAnd by opposing end them", token_ids=(3488, 198, 1271, 387, 477, 539, 311, 387, 11, 430, 374, 279, 3488, 198, 25729, 364, 83, 285, 13348, 1565, 304, 279, 4059, 311, 7831, 198, 791, 1776, 826, 323, 38057, 315, 55588, 33415, 198, 2244, 311, 1935, 11977, 2403, 264, 9581, 315, 35665, 198, 3112, 555, 31322, 842, 1124), cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1753251561.8060884, last_token_time=1753251635.8677995, first_scheduled_time=1753251561.8102646, first_token_time=1753251562.669284, time_in_queue=0.004176139831542969, finished_time=1753251635.8680444, scheduler_time=0.015669081010855734, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})
NOT HERE!!!!!! free_finished_seq_group
NOT HERE!!!!!! free_finished_seqs
FREEING SEQ 0
Generated:  question
To be or not to be, that is the question
Whether 'tis nobler in the mind to suffer
The slings and arrows of outrageous fortune
Or to take arms against a sea of troubles
And by opposing end them
