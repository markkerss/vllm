INFO 06-25 05:45:10 [__init__.py:239] Automatically detected platform cuda.
INFO 06-25 05:45:19 [config.py:604] This model supports multiple tasks: {'score', 'embed', 'classify', 'generate', 'reward'}. Defaulting to 'generate'.
WARNING 06-25 05:45:19 [arg_utils.py:1607] Chunked prefill is enabled by default for models with max_model_len > 32K. Chunked prefill might not work with some features or models. If you encounter any issues, please disable by launching with --enable-chunked-prefill=False.
INFO 06-25 05:45:19 [config.py:1797] Chunked prefill is enabled with max_num_batched_tokens=2048.
INFO 06-25 05:45:19 [llm_engine.py:248] Initializing a V0 LLM engine (v0.8.3rc2.dev74+gae842e64.d20250622) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 06-25 05:45:21 [cuda.py:292] Using Flash Attention backend.
INFO 06-25 05:45:21 [parallel_state.py:957] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 06-25 05:45:21 [model_runner.py:1157] Starting to load model meta-llama/Llama-3.1-8B-Instruct...
INFO 06-25 05:45:22 [weight_utils.py:265] Using model weights format ['*.safetensors']
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:02,  1.25it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.93it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.50it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.32it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.40it/s]

INFO 06-25 05:45:25 [loader.py:458] Loading weights took 3.00 seconds
INFO 06-25 05:45:25 [model_runner.py:1193] Model loading took 14.9889 GiB and 3.408680 seconds
INFO 06-25 05:45:25 [model_runner.py:1267] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='0', is_prompt=True, seq_data={0: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='1', is_prompt=True, seq_data={1: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='2', is_prompt=True, seq_data={2: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='3', is_prompt=True, seq_data={3: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='4', is_prompt=True, seq_data={4: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='5', is_prompt=True, seq_data={5: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='6', is_prompt=True, seq_data={6: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='7', is_prompt=True, seq_data={7: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='8', is_prompt=True, seq_data={8: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='9', is_prompt=True, seq_data={9: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='10', is_prompt=True, seq_data={10: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='11', is_prompt=True, seq_data={11: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='12', is_prompt=True, seq_data={12: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='13', is_prompt=True, seq_data={13: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='14', is_prompt=True, seq_data={14: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='15', is_prompt=True, seq_data={15: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='16', is_prompt=True, seq_data={16: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='17', is_prompt=True, seq_data={17: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='18', is_prompt=True, seq_data={18: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='19', is_prompt=True, seq_data={19: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='20', is_prompt=True, seq_data={20: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='21', is_prompt=True, seq_data={21: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='22', is_prompt=True, seq_data={22: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='23', is_prompt=True, seq_data={23: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='24', is_prompt=True, seq_data={24: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='25', is_prompt=True, seq_data={25: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='26', is_prompt=True, seq_data={26: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='27', is_prompt=True, seq_data={27: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='28', is_prompt=True, seq_data={28: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='29', is_prompt=True, seq_data={29: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='30', is_prompt=True, seq_data={30: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='31', is_prompt=True, seq_data={31: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='32', is_prompt=True, seq_data={32: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='33', is_prompt=True, seq_data={33: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='34', is_prompt=True, seq_data={34: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='35', is_prompt=True, seq_data={35: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='36', is_prompt=True, seq_data={36: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='37', is_prompt=True, seq_data={37: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='38', is_prompt=True, seq_data={38: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='39', is_prompt=True, seq_data={39: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='40', is_prompt=True, seq_data={40: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='41', is_prompt=True, seq_data={41: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='42', is_prompt=True, seq_data={42: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='43', is_prompt=True, seq_data={43: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='44', is_prompt=True, seq_data={44: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='45', is_prompt=True, seq_data={45: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='46', is_prompt=True, seq_data={46: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='47', is_prompt=True, seq_data={47: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='48', is_prompt=True, seq_data={48: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='49', is_prompt=True, seq_data={49: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='50', is_prompt=True, seq_data={50: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='51', is_prompt=True, seq_data={51: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='52', is_prompt=True, seq_data={52: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='53', is_prompt=True, seq_data={53: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='54', is_prompt=True, seq_data={54: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='55', is_prompt=True, seq_data={55: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='56', is_prompt=True, seq_data={56: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='57', is_prompt=True, seq_data={57: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='58', is_prompt=True, seq_data={58: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='59', is_prompt=True, seq_data={59: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='60', is_prompt=True, seq_data={60: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='61', is_prompt=True, seq_data={61: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='62', is_prompt=True, seq_data={62: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='63', is_prompt=True, seq_data={63: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='64', is_prompt=True, seq_data={64: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='65', is_prompt=True, seq_data={65: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='66', is_prompt=True, seq_data={66: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='67', is_prompt=True, seq_data={67: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='68', is_prompt=True, seq_data={68: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='69', is_prompt=True, seq_data={69: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='70', is_prompt=True, seq_data={70: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='71', is_prompt=True, seq_data={71: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='72', is_prompt=True, seq_data={72: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='73', is_prompt=True, seq_data={73: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='74', is_prompt=True, seq_data={74: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='75', is_prompt=True, seq_data={75: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='76', is_prompt=True, seq_data={76: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='77', is_prompt=True, seq_data={77: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='78', is_prompt=True, seq_data={78: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='79', is_prompt=True, seq_data={79: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='80', is_prompt=True, seq_data={80: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='81', is_prompt=True, seq_data={81: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='82', is_prompt=True, seq_data={82: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='83', is_prompt=True, seq_data={83: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='84', is_prompt=True, seq_data={84: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='85', is_prompt=True, seq_data={85: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='86', is_prompt=True, seq_data={86: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='87', is_prompt=True, seq_data={87: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='88', is_prompt=True, seq_data={88: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='89', is_prompt=True, seq_data={89: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='90', is_prompt=True, seq_data={90: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='91', is_prompt=True, seq_data={91: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='92', is_prompt=True, seq_data={92: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='93', is_prompt=True, seq_data={93: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='94', is_prompt=True, seq_data={94: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='95', is_prompt=True, seq_data={95: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='96', is_prompt=True, seq_data={96: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='97', is_prompt=True, seq_data={97: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='98', is_prompt=True, seq_data={98: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='99', is_prompt=True, seq_data={99: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='100', is_prompt=True, seq_data={100: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='101', is_prompt=True, seq_data={101: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='102', is_prompt=True, seq_data={102: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='103', is_prompt=True, seq_data={103: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='104', is_prompt=True, seq_data={104: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='105', is_prompt=True, seq_data={105: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='106', is_prompt=True, seq_data={106: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='107', is_prompt=True, seq_data={107: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='108', is_prompt=True, seq_data={108: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='109', is_prompt=True, seq_data={109: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='110', is_prompt=True, seq_data={110: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='111', is_prompt=True, seq_data={111: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='112', is_prompt=True, seq_data={112: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='113', is_prompt=True, seq_data={113: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='114', is_prompt=True, seq_data={114: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='115', is_prompt=True, seq_data={115: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='116', is_prompt=True, seq_data={116: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='117', is_prompt=True, seq_data={117: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='118', is_prompt=True, seq_data={118: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='119', is_prompt=True, seq_data={119: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='120', is_prompt=True, seq_data={120: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='121', is_prompt=True, seq_data={121: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='122', is_prompt=True, seq_data={122: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='123', is_prompt=True, seq_data={123: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='124', is_prompt=True, seq_data={124: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='125', is_prompt=True, seq_data={125: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='126', is_prompt=True, seq_data={126: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='127', is_prompt=True, seq_data={127: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='128', is_prompt=True, seq_data={128: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='129', is_prompt=True, seq_data={129: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='130', is_prompt=True, seq_data={130: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='131', is_prompt=True, seq_data={131: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='132', is_prompt=True, seq_data={132: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='133', is_prompt=True, seq_data={133: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='134', is_prompt=True, seq_data={134: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='135', is_prompt=True, seq_data={135: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='136', is_prompt=True, seq_data={136: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='137', is_prompt=True, seq_data={137: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='138', is_prompt=True, seq_data={138: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='139', is_prompt=True, seq_data={139: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='140', is_prompt=True, seq_data={140: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='141', is_prompt=True, seq_data={141: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='142', is_prompt=True, seq_data={142: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='143', is_prompt=True, seq_data={143: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='144', is_prompt=True, seq_data={144: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='145', is_prompt=True, seq_data={145: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='146', is_prompt=True, seq_data={146: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='147', is_prompt=True, seq_data={147: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='148', is_prompt=True, seq_data={148: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='149', is_prompt=True, seq_data={149: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='150', is_prompt=True, seq_data={150: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='151', is_prompt=True, seq_data={151: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='152', is_prompt=True, seq_data={152: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='153', is_prompt=True, seq_data={153: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='154', is_prompt=True, seq_data={154: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='155', is_prompt=True, seq_data={155: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='156', is_prompt=True, seq_data={156: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='157', is_prompt=True, seq_data={157: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='158', is_prompt=True, seq_data={158: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='159', is_prompt=True, seq_data={159: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='160', is_prompt=True, seq_data={160: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='161', is_prompt=True, seq_data={161: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='162', is_prompt=True, seq_data={162: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='163', is_prompt=True, seq_data={163: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='164', is_prompt=True, seq_data={164: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='165', is_prompt=True, seq_data={165: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='166', is_prompt=True, seq_data={166: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='167', is_prompt=True, seq_data={167: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='168', is_prompt=True, seq_data={168: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='169', is_prompt=True, seq_data={169: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='170', is_prompt=True, seq_data={170: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='171', is_prompt=True, seq_data={171: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='172', is_prompt=True, seq_data={172: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='173', is_prompt=True, seq_data={173: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='174', is_prompt=True, seq_data={174: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='175', is_prompt=True, seq_data={175: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='176', is_prompt=True, seq_data={176: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='177', is_prompt=True, seq_data={177: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='178', is_prompt=True, seq_data={178: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='179', is_prompt=True, seq_data={179: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='180', is_prompt=True, seq_data={180: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='181', is_prompt=True, seq_data={181: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='182', is_prompt=True, seq_data={182: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='183', is_prompt=True, seq_data={183: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='184', is_prompt=True, seq_data={184: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='185', is_prompt=True, seq_data={185: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='186', is_prompt=True, seq_data={186: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='187', is_prompt=True, seq_data={187: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='188', is_prompt=True, seq_data={188: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='189', is_prompt=True, seq_data={189: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='190', is_prompt=True, seq_data={190: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='191', is_prompt=True, seq_data={191: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='192', is_prompt=True, seq_data={192: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='193', is_prompt=True, seq_data={193: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='194', is_prompt=True, seq_data={194: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='195', is_prompt=True, seq_data={195: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='196', is_prompt=True, seq_data={196: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='197', is_prompt=True, seq_data={197: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='198', is_prompt=True, seq_data={198: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='199', is_prompt=True, seq_data={199: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='200', is_prompt=True, seq_data={200: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='201', is_prompt=True, seq_data={201: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='202', is_prompt=True, seq_data={202: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='203', is_prompt=True, seq_data={203: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='204', is_prompt=True, seq_data={204: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='205', is_prompt=True, seq_data={205: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='206', is_prompt=True, seq_data={206: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='207', is_prompt=True, seq_data={207: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='208', is_prompt=True, seq_data={208: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='209', is_prompt=True, seq_data={209: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='210', is_prompt=True, seq_data={210: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='211', is_prompt=True, seq_data={211: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='212', is_prompt=True, seq_data={212: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='213', is_prompt=True, seq_data={213: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='214', is_prompt=True, seq_data={214: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='215', is_prompt=True, seq_data={215: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='216', is_prompt=True, seq_data={216: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='217', is_prompt=True, seq_data={217: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='218', is_prompt=True, seq_data={218: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='219', is_prompt=True, seq_data={219: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='220', is_prompt=True, seq_data={220: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='221', is_prompt=True, seq_data={221: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='222', is_prompt=True, seq_data={222: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='223', is_prompt=True, seq_data={223: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='224', is_prompt=True, seq_data={224: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='225', is_prompt=True, seq_data={225: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='226', is_prompt=True, seq_data={226: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='227', is_prompt=True, seq_data={227: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='228', is_prompt=True, seq_data={228: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='229', is_prompt=True, seq_data={229: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='230', is_prompt=True, seq_data={230: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='231', is_prompt=True, seq_data={231: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='232', is_prompt=True, seq_data={232: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='233', is_prompt=True, seq_data={233: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='234', is_prompt=True, seq_data={234: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='235', is_prompt=True, seq_data={235: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='236', is_prompt=True, seq_data={236: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='237', is_prompt=True, seq_data={237: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='238', is_prompt=True, seq_data={238: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='239', is_prompt=True, seq_data={239: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='240', is_prompt=True, seq_data={240: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='241', is_prompt=True, seq_data={241: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='242', is_prompt=True, seq_data={242: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='243', is_prompt=True, seq_data={243: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='244', is_prompt=True, seq_data={244: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='245', is_prompt=True, seq_data={245: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='246', is_prompt=True, seq_data={246: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='247', is_prompt=True, seq_data={247: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='248', is_prompt=True, seq_data={248: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='249', is_prompt=True, seq_data={249: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='250', is_prompt=True, seq_data={250: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='251', is_prompt=True, seq_data={251: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='252', is_prompt=True, seq_data={252: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='253', is_prompt=True, seq_data={253: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='254', is_prompt=True, seq_data={254: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None), SequenceGroupMetadata(request_id='255', is_prompt=True, seq_data={255: SequenceData(prompt_token_ids=array('l', [0, 0, 0, 0, 0, 0, 0, 0]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=0.99, top_k=128255, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables=None, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=None, state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=None, multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs=None, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=8, num_speculative_tokens=None)]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 0, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 0 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 1, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 1 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG: _compute_lens for req_id 1, seq_id 1
  context_len: 0, seq_len (for step): 8, token_chunk_size: 8
  Calculated tokens for this step: [0, 0, 0, 0, 0, 0, 0, 0] (len: 8)
  Calculated positions for this step: list(range(0, 8)) (len: 8)
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 2, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 2 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 3, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 3 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 4, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 4 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 5, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 5 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 6, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 6 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 7, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 7 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 8, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 8 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 9, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 9 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 10, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 10 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 11, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 11 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 12, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 12 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 13, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 13 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 14, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 14 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 15, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 15 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 16, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 16 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 17, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 17 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 18, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 18 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 19, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 19 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 20, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 20 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 21, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 21 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 22, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 22 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 23, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 23 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 24, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 24 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 25, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 25 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 26, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 26 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 27, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 27 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 28, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 28 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 29, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 29 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 30, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 30 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 31, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 31 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 32, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 32 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 33, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 33 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 34, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 34 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 35, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 35 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 36, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 36 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 37, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 37 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 38, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 38 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 39, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 39 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 40, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 40 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 41, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 41 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 42, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 42 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 43, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 43 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 44, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 44 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 45, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 45 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 46, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 46 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 47, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 47 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 48, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 48 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 49, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 49 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 50, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 50 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 51, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 51 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 52, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 52 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 53, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 53 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 54, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 54 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 55, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 55 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 56, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 56 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 57, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 57 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 58, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 58 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 59, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 59 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 60, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 60 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 61, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 61 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 62, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 62 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 63, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 63 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 64, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 64 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 65, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 65 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 66, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 66 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 67, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 67 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 68, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 68 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 69, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 69 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 70, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 70 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 71, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 71 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 72, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 72 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 73, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 73 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 74, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 74 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 75, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 75 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 76, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 76 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 77, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 77 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 78, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 78 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 79, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 79 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 80, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 80 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 81, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 81 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 82, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 82 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 83, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 83 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 84, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 84 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 85, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 85 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 86, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 86 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 87, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 87 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 88, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 88 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 89, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 89 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 90, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 90 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 91, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 91 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 92, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 92 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 93, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 93 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 94, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 94 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 95, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 95 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 96, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 96 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 97, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 97 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 98, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 98 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 99, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 99 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 100, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 100 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 101, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 101 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 102, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 102 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 103, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 103 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 104, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 104 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 105, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 105 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 106, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 106 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 107, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 107 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 108, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 108 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 109, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 109 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 110, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 110 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 111, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 111 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 112, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 112 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 113, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 113 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 114, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 114 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 115, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 115 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 116, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 116 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 117, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 117 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 118, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 118 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 119, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 119 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 120, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 120 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 121, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 121 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 122, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 122 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 123, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 123 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 124, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 124 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 125, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 125 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 126, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 126 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 127, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 127 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 128, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 128 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 129, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 129 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 130, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 130 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 131, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 131 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 132, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 132 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 133, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 133 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 134, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 134 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 135, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 135 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 136, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 136 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 137, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 137 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 138, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 138 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 139, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 139 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 140, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 140 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 141, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 141 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 142, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 142 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 143, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 143 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 144, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 144 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 145, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 145 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 146, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 146 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 147, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 147 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 148, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 148 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 149, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 149 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 150, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 150 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 151, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 151 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 152, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 152 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 153, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 153 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 154, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 154 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 155, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 155 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 156, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 156 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 157, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 157 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 158, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 158 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 159, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 159 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 160, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 160 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 161, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 161 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 162, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 162 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 163, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 163 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 164, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 164 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 165, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 165 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 166, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 166 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 167, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 167 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 168, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 168 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 169, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 169 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 170, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 170 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 171, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 171 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 172, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 172 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 173, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 173 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 174, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 174 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 175, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 175 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 176, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 176 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 177, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 177 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 178, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 178 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 179, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 179 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 180, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 180 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 181, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 181 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 182, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 182 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 183, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 183 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 184, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 184 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 185, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 185 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 186, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 186 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 187, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 187 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 188, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 188 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 189, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 189 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 190, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 190 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 191, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 191 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 192, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 192 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 193, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 193 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 194, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 194 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 195, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 195 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 196, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 196 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 197, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 197 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 198, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 198 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 199, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 199 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 200, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 200 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 201, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 201 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 202, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 202 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 203, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 203 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 204, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 204 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 205, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 205 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 206, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 206 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 207, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 207 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 208, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 208 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 209, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 209 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 210, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 210 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 211, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 211 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 212, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 212 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 213, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 213 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 214, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 214 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 215, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 215 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 216, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 216 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 217, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 217 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 218, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 218 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 219, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 219 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 220, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 220 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 221, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 221 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 222, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 222 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 223, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 223 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 224, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 224 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 225, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 225 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 226, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 226 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 227, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 227 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 228, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 228 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 229, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 229 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 230, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 230 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 231, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 231 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 232, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 232 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 233, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 233 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 234, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 234 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 235, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 235 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 236, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 236 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 237, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 237 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 238, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 238 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 239, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 239 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 240, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 240 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 241, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 241 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 242, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 242 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 243, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 243 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 244, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 244 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 245, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 245 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 246, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 246 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 247, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 247 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 248, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 248 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 249, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 249 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 250, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 250 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 251, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 251 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 252, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 252 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 253, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 253 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 254, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 254 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
INFO 06-25 05:45:25 [model_runner.py:1270] Processing seq group 255, is_prompt=True, multi_modal_data=False
MAX TOKENS 16
SEQ DATA INSIDE COMPUTE LENS 255 (0, 0, 0, 0, 0, 0, 0, 0)
TOKEN CHUNK SIZE 8
SEQ_DATA INSIDE WORKER 0 8 [0, 0, 0, 0, 0, 0, 0, 0]
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 1):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [0, 0, 0, 0, 0, 0, 0, 0] (len: 8)
      Seq 0: input_positions in list: [0, 1, 2, 3, 4, 5, 6, 7] (len: 8)
      Seq 0: Expected segment in FLAT input_tokens: offset 8, len 8
      Seq 0: Expected segment in FLAT input_positions: offset 8, len 8
  Total flattened input_tokens (pre-padding): [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] (len: 2048)
INFO 06-25 05:45:26 [worker.py:269] Memory profiling takes 1.08 seconds
INFO 06-25 05:45:26 [worker.py:269] the current vLLM instance can use total_gpu_memory (79.15GiB) x gpu_memory_utilization (0.90) = 71.24GiB
INFO 06-25 05:45:26 [worker.py:269] model weights take 14.99GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 1.19GiB; the rest of the memory reserved for KV Cache is 54.97GiB.
INFO 06-25 05:45:26 [executor_base.py:112] # cuda blocks: 28143, # CPU blocks: 2048
INFO 06-25 05:45:26 [executor_base.py:117] Maximum concurrency for 131072 tokens per request: 3.44x
INFO 06-25 05:45:28 [model_runner.py:1514] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]Capturing CUDA graph shapes:   3%|         | 1/35 [00:00<00:13,  2.61it/s]Capturing CUDA graph shapes:   6%|         | 2/35 [00:00<00:12,  2.68it/s]Capturing CUDA graph shapes:   9%|         | 3/35 [00:01<00:12,  2.64it/s]Capturing CUDA graph shapes:  11%|        | 4/35 [00:01<00:11,  2.65it/s]Capturing CUDA graph shapes:  14%|        | 5/35 [00:01<00:11,  2.67it/s]Capturing CUDA graph shapes:  17%|        | 6/35 [00:02<00:10,  2.68it/s]Capturing CUDA graph shapes:  20%|        | 7/35 [00:02<00:10,  2.69it/s]Capturing CUDA graph shapes:  23%|       | 8/35 [00:02<00:10,  2.69it/s]Capturing CUDA graph shapes:  26%|       | 9/35 [00:03<00:09,  2.70it/s]Capturing CUDA graph shapes:  29%|       | 10/35 [00:03<00:09,  2.71it/s]Capturing CUDA graph shapes:  31%|      | 11/35 [00:04<00:08,  2.72it/s]Capturing CUDA graph shapes:  34%|      | 12/35 [00:04<00:08,  2.72it/s]Capturing CUDA graph shapes:  37%|      | 13/35 [00:04<00:08,  2.72it/s]Capturing CUDA graph shapes:  40%|      | 14/35 [00:05<00:07,  2.71it/s]Capturing CUDA graph shapes:  43%|     | 15/35 [00:05<00:07,  2.73it/s]Capturing CUDA graph shapes:  46%|     | 16/35 [00:05<00:06,  2.72it/s]Capturing CUDA graph shapes:  49%|     | 17/35 [00:06<00:06,  2.75it/s]Capturing CUDA graph shapes:  51%|    | 18/35 [00:06<00:06,  2.72it/s]Capturing CUDA graph shapes:  54%|    | 19/35 [00:07<00:05,  2.74it/s]Capturing CUDA graph shapes:  57%|    | 20/35 [00:07<00:05,  2.74it/s]Capturing CUDA graph shapes:  60%|    | 21/35 [00:07<00:05,  2.76it/s]Capturing CUDA graph shapes:  63%|   | 22/35 [00:08<00:04,  2.76it/s]Capturing CUDA graph shapes:  66%|   | 23/35 [00:08<00:04,  2.79it/s]Capturing CUDA graph shapes:  69%|   | 24/35 [00:08<00:03,  2.77it/s]Capturing CUDA graph shapes:  71%|  | 25/35 [00:09<00:03,  2.80it/s]Capturing CUDA graph shapes:  74%|  | 26/35 [00:09<00:03,  2.80it/s]Capturing CUDA graph shapes:  77%|  | 27/35 [00:09<00:02,  2.81it/s]Capturing CUDA graph shapes:  80%|  | 28/35 [00:10<00:02,  2.82it/s]Capturing CUDA graph shapes:  83%| | 29/35 [00:10<00:02,  2.84it/s]Capturing CUDA graph shapes:  86%| | 30/35 [00:10<00:01,  2.81it/s]Capturing CUDA graph shapes:  89%| | 31/35 [00:11<00:01,  2.83it/s]Capturing CUDA graph shapes:  91%|| 32/35 [00:11<00:01,  2.83it/s]Capturing CUDA graph shapes:  94%|| 33/35 [00:11<00:00,  2.82it/s]Capturing CUDA graph shapes:  97%|| 34/35 [00:12<00:00,  2.82it/s]Capturing CUDA graph shapes: 100%|| 35/35 [00:12<00:00,  2.83it/s]Capturing CUDA graph shapes: 100%|| 35/35 [00:12<00:00,  2.76it/s]
INFO 06-25 05:45:41 [model_runner.py:1656] Graph capturing finished in 13 secs, took 0.88 GiB
INFO 06-25 05:45:41 [llm_engine.py:454] init engine (profile, create kv cache, warmup model) took 15.64 seconds
run_first_add_chunk
SCHEDULING NOT IMPORTED
New tokens to schedule 4
ALLOCATED AND SET RUNNING
SCEDULING THE GROUP
SENDING FULL METADATA (128000, 9906, 11, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 9906, 11, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 9906, 11, 220)
INFO 06-25 05:45:41 [model_runner.py:1267] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=True, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 9906, 11, 220]), output_token_ids=(), cumulative_logprob=0.0, get_num_computed_tokens=0)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=[], multi_modal_data={}, multi_modal_placeholders={}, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=4, num_speculative_tokens=None)]
INFO 06-25 05:45:41 [model_runner.py:1270] Processing seq group 1, is_prompt=True, multi_modal_data=True
MAX TOKENS 1
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 9906, 11, 220)
TOKEN CHUNK SIZE 4
SEQ_DATA INSIDE WORKER 0 4 [128000, 9906, 11, 220]
DEBUG: _compute_lens for req_id 1, seq_id 0
  context_len: 0, seq_len (for step): 4, token_chunk_size: 4
  Calculated tokens for this step: [128000, 9906, 11, 220] (len: 4)
  Calculated positions for this step: list(range(0, 4)) (len: 4)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [128000, 9906, 11, 220] (len: 4)
      Seq 0: input_positions in list: [0, 1, 2, 3] (len: 4)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 4
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 4
  Total flattened input_tokens (pre-padding): [128000, 9906, 11, 220] (len: 4)
APPENDED NEW TOKENS array('l', [128000, 9906, 11, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 9906, 11, 220)
(128000, 9906, 11, 220)
(128000, 9906, 11, 220)
SENDING FULL METADATA (128000, 9906, 11, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 9906, 11, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 9906, 11, 220)
INFO 06-25 05:45:41 [model_runner.py:1267] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 9906, 11, 220]), output_token_ids=(679,), cumulative_logprob=inf, get_num_computed_tokens=4)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=SequenceGroupState(num_steps=1, current_step=0), token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 06-25 05:45:41 [model_runner.py:1270] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 1
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 9906, 11, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 4 5 [128000, 9906, 11, 220, 679]
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [679] (len: 1)
      Seq 0: input_positions in list: [4] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [679] (len: 1)
COMPUTED TOKENS: 4 
OUTPUT LEN: 1 
LEN: 5 
PROMPT LEN: 4
COMPUTED TOKENS: 4 
OUTPUT LEN: 0 
LEN: 4 
PROMPT LEN: 4
{'1': SequenceGroup(request_id=1, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), num_seqs=1)} 1
SUSPENDED THE GROUP!!!
SEQ ID 0
INFO 06-25 05:45:46 [metrics.py:489] Avg prompt throughput: 0.8 tokens/s, Avg generation throughput: 0.2 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
run_add_chunk
Request id 1
Suspended requests {'1': SequenceGroup(request_id=1, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), num_seqs=1)}
WE ARE ADDING [128000, 2465, 836, 374, 220] TO SEQ ID 0
COMPUTED TOKENS: 4 
OUTPUT LEN: 0 
LEN: 9 
PROMPT LEN: 9
APPENDED NEW TOKENS array('l', [128000, 9906, 11, 220, 128000, 2465, 836, 374, 220])
SEQ ID 0
New tokens to schedule 5
SEQUENCE IS PREFILL True
(128000, 9906, 11, 220, 128000, 2465, 836, 374, 220)
(128000, 9906, 11, 220, 128000, 2465, 836, 374, 220)
(128000, 9906, 11, 220, 128000, 2465, 836, 374, 220)
CHECKING SCHEDULED SEQ GROUP CONTENTS array('l', [128000, 9906, 11, 220, 128000, 2465, 836, 374, 220])
SCHEDULED SEQ GROUP CONTENTS (128000, 9906, 11, 220, 128000, 2465, 836, 374, 220)
SCHEDULING PREFILL
RUNNING GROUP PREFILLS BEFORE RETURN (128000, 9906, 11, 220, 128000, 2465, 836, 374, 220)
RUNNING GROUP PREFILLS (128000, 9906, 11, 220, 128000, 2465, 836, 374, 220)
SENDING FULL METADATA (128000, 9906, 11, 220, 128000, 2465, 836, 374, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 9906, 11, 220, 128000, 2465, 836, 374, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 9906, 11, 220, 128000, 2465, 836, 374, 220)
INFO 06-25 05:45:46 [model_runner.py:1267] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=True, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 9906, 11, 220, 128000, 2465, 836, 374, 220]), output_token_ids=(), cumulative_logprob=inf, get_num_computed_tokens=4)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.PREFILL: 1>, token_type_ids=[], multi_modal_data={}, multi_modal_placeholders={}, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=5, num_speculative_tokens=None)]
INFO 06-25 05:45:46 [model_runner.py:1270] Processing seq group 1, is_prompt=True, multi_modal_data=True
MAX TOKENS 1
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 9906, 11, 220, 128000, 2465, 836, 374, 220)
TOKEN CHUNK SIZE 5
SEQ_DATA INSIDE WORKER 4 9 [128000, 9906, 11, 220, 128000, 2465, 836, 374, 220]
DEBUG: _compute_lens for req_id 1, seq_id 0
  context_len: 4, seq_len (for step): 9, token_chunk_size: 5
  Calculated tokens for this step: [128000, 2465, 836, 374, 220] (len: 5)
  Calculated positions for this step: list(range(4, 9)) (len: 5)
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [128000, 2465, 836, 374, 220] (len: 5)
      Seq 0: input_positions in list: [4, 5, 6, 7, 8] (len: 5)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 5
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 5
  Total flattened input_tokens (pre-padding): [128000, 2465, 836, 374, 220] (len: 5)
APPENDED NEW TOKENS array('l', [128000, 9906, 11, 220, 128000, 2465, 836, 374, 220])
SEQ ID 0
UNCACHED NEW TOKENS 1
SEQUENCE IS PREFILL False
(128000, 9906, 11, 220, 128000, 2465, 836, 374, 220)
(128000, 9906, 11, 220, 128000, 2465, 836, 374, 220)
(128000, 9906, 11, 220, 128000, 2465, 836, 374, 220)
SENDING FULL METADATA (128000, 9906, 11, 220, 128000, 2465, 836, 374, 220) 1
SEQ_DATA INSIDE ENGINE TO SEND 0 (128000, 9906, 11, 220, 128000, 2465, 836, 374, 220)
SEQ DATA PREPARE WORKER INPUT 0 (128000, 9906, 11, 220, 128000, 2465, 836, 374, 220)
INFO 06-25 05:45:46 [model_runner.py:1267] Preparing model input tensors. seq_group_metadata_list: [SequenceGroupMetadata(request_id='1', is_prompt=False, seq_data={0: SequenceData(prompt_token_ids=array('l', [128000, 9906, 11, 220, 128000, 2465, 836, 374, 220]), output_token_ids=(35266,), cumulative_logprob=inf, get_num_computed_tokens=9)}, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), block_tables={0: [0]}, do_sample=True, pooling_params=None, lora_request=None, computed_block_nums=[], state=<SequenceStage.PREFILL: 1>, token_type_ids=[], multi_modal_data=None, multi_modal_placeholders=None, mm_processor_kwargs={}, encoder_seq_data=None, cross_block_table=None, prompt_adapter_request=None, token_chunk_size=1, num_speculative_tokens=None)]
INFO 06-25 05:45:46 [model_runner.py:1270] Processing seq group 1, is_prompt=False, multi_modal_data=False
MAX TOKENS 1
SEQ DATA INSIDE COMPUTE LENS 0 (128000, 9906, 11, 220, 128000, 2465, 836, 374, 220)
TOKEN CHUNK SIZE 1
SEQ_DATA INSIDE WORKER 9 10 [128000, 9906, 11, 220, 128000, 2465, 836, 374, 220, 35266]
DEBUG: build() - Before tensor conversion for req_id 1 (present in batch):
  Req '1' (inter_data_list index 0):
    Number of sequences in this group: 1
      Seq 0: input_tokens in list: [35266] (len: 1)
      Seq 0: input_positions in list: [9] (len: 1)
      Seq 0: Expected segment in FLAT input_tokens: offset 0, len 1
      Seq 0: Expected segment in FLAT input_positions: offset 0, len 1
  Total flattened input_tokens (pre-padding): [35266] (len: 1)
COMPUTED TOKENS: 9 
OUTPUT LEN: 1 
LEN: 10 
PROMPT LEN: 9
COMPUTED TOKENS: 9 
OUTPUT LEN: 0 
LEN: 9 
PROMPT LEN: 9
{'1': SequenceGroup(request_id=1, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[128008, 128001], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), num_seqs=1)} 1
SUSPENDED THE GROUP!!!
SEQ ID 0
INFO 06-25 05:45:51 [metrics.py:489] Avg prompt throughput: 1.0 tokens/s, Avg generation throughput: 0.2 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
deque([])
run_decode
deque([])
[rank0]:[W625 05:45:52.267447500 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
